# RAG System â€“ Copilot Development Instructions

## ğŸ§  Overview
This repository implements a modular **Retrieval-Augmented Generation (RAG)** system designed with **Object-Oriented Programming (OOP)** principles.

The pipeline includes:
1. **Loader Module** â†’ Handles PDF/DOCX ingestion and normalization. âœ… *(Completed)*
2. **Chunker Module** â†’ Handles text segmentation into embedding-ready chunks. ğŸš§ *(Current focus)*

Goal: Transform PDF/DOCX â†’ `NormalizedDocument` â†’ `ChunkSet` â†’ (Embedding â†’ Reranking â†’ Retrieval).

---

## âš™ï¸ Environment Setup
```powershell
# Activate virtual environment
& C:/Users/ENGUYEHWC/Downloads/RAG/RAG/.venv/Scripts/Activate.ps1

# Install dependencies
pip install -r requirements.txt
ğŸ“ Project Structure
bash
RAG/
â”œâ”€â”€ .venv/                         # Virtual environment
â”œâ”€â”€ loaders/                       # âœ… PDF/DOCX ingestion
â”‚   â”œâ”€â”€ pdf_loader.py
â”‚   â”œâ”€â”€ model/
â”‚   â””â”€â”€ normalizers/
â”œâ”€â”€ chunkers/                      # ğŸš§ Current focus
â”‚   â”œâ”€â”€ hybrid_chunker.py          # Main orchestrator
â”‚   â”œâ”€â”€ semantic_chunker.py        # Semantic segmentation
â”‚   â”œâ”€â”€ rule_chunker.py            # Rule-based segmentation
â”‚   â”œâ”€â”€ fixed_chunker.py           # Fixed-length fallback
â”‚   â”œâ”€â”€ model.py                   # Shared data classes
â”‚   â””â”€â”€ utils.py                   # Token estimator & helpers
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_loader.py
â”‚   â””â”€â”€ test_chunker.py
â””â”€â”€ copilot-instruc.md             # Copilot & developer guidance
ğŸ“š Loader Module Summary (âœ… Completed)
File: loaders/pdf_loader.py
Purpose: Extract and normalize PDF/DOCX files.

Output schema:

python
Sao chÃ©p mÃ£
NormalizedDocument {
  documentId: UUID,
  metadata: {...},
  blocks: [
    {
      blockId: UUID,
      type: "paragraph" | "heading" | "list" | "table" | "code",
      text: str,
      provenance: { file, page, charRange }
    }
  ]
}
Key Features

Dependency injection (config via constructor)

Text & table extraction

Config validation

Factory methods: create_default(), create_text_only(), create_tables_only()

OOP encapsulation and static utilities

ğŸ”§ Chunker Module (ğŸš§ Current Focus)
ğŸ¯ Objective
Convert a normalized document into semantically meaningful chunks.
Implements Hybrid Chunking: combining semantic, rule-based, and fixed-size strategies.

ğŸ§© Class Overview
Class	Responsibility
HybridChunker	Main orchestrator; selects and manages strategies
SemanticChunker	Semantic segmentation using text coherence or embeddings
RuleBasedChunker	Structural segmentation by headings, lists, tables
FixedSizeChunker	Token-length fallback segmentation
ChunkSet	Holds list of chunks for a document
Chunk	Represents one embedding-ready segment
ProvenanceAgg	Aggregates provenance from all contributing blocks
BlockSpan	Represents character offsets within source blocks
Score	Chunk quality metrics
ChunkStats	Aggregated chunking statistics

âš™ï¸ Architecture Flow
css

NormalizedDocument
      â†“
 HybridChunker
 â”œâ”€ SemanticChunker
 â”œâ”€ RuleBasedChunker
 â””â”€ FixedSizeChunker
      â†“
   ChunkSet
    â””â”€â”€ [Chunk â†’ ProvenanceAgg â†’ BlockSpan]
ğŸ§  HybridChunker Parameters
python
HybridChunker(
  targetTokens=200,
  minTokens=100,
  maxTokens=400,
  overlapRatio=0.1,
  language="en"
)
ğŸ’¡ Core Methods
Method	Description
HybridChunker.chunk(doc)	Entry point; orchestrates all strategies
HybridChunker.evaluateAndRefine(set)	Optional QA step
SemanticChunker.chunkSegment(blocks)	Splits by semantic boundaries
RuleBasedChunker.chunkByRules(blocks)	Splits by structural rules
FixedSizeChunker.chunkByLength(blocks)	Splits evenly by token length

ğŸ§± Data Models (chunkers/model.py)
python
@dataclass
class BlockSpan:
    blockId: str
    charStart: int
    charEnd: int
    bbox: Optional[str] = None

@dataclass
class ProvenanceAgg:
    file: str
    sha256Doc: str
    pageRanges: List[int]
    blockSpans: List[BlockSpan]

@dataclass
class Score:
    cohesion: float = 0
    topicShift: float = 0
    structureConf: float = 0
    boundaryConf: float = 0

@dataclass
class Chunk:
    chunkId: str
    order: int
    textForEmbedding: str
    tokensEstimate: int
    contentType: str
    scores: Score
    provenance: ProvenanceAgg

@dataclass
class ChunkStats:
    numChunks: int
    avgTokens: int
    stdevTokens: float

@dataclass
class ChunkSet:
    documentId: str
    chunks: List[Chunk]
    stats: Optional[ChunkStats] = None