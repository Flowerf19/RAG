# Migration Guide - Cáº¥u trÃºc má»›i

## ğŸ“‹ Tá»•ng quan thay Ä‘á»•i

Há»‡ thá»‘ng Ä‘Ã£ Ä‘Æ°á»£c refactor hoÃ n toÃ n theo nguyÃªn táº¯c OOP vÃ  Single Responsibility:

```
CÅ¨ (llm/):                          Má»šI (ui/ + llm/):
â”œâ”€â”€ LLM_FE.py (UI+LLM)    â†’        ui/
â”œâ”€â”€ LLM_API.py            â†’          â”œâ”€â”€ app.py (Main Streamlit)
â”œâ”€â”€ LLM_LOCAL.py          â†’          â””â”€â”€ components/ (UI components)
â”œâ”€â”€ chat_handler.py       â†’        
â”œâ”€â”€ chat_styles.css       â†’        llm/
â””â”€â”€ config_loader.py      â†’          â”œâ”€â”€ base_client.py (Abstract)
                                     â”œâ”€â”€ gemini_client.py
                                     â”œâ”€â”€ lmstudio_client.py
                                     â”œâ”€â”€ client_factory.py
                                     â”œâ”€â”€ chat_handler.py (unchanged)
                                     â””â”€â”€ config_loader.py (unchanged)
```

## ğŸš€ CÃ¡ch cháº¡y UI má»›i

### CÅ©
```powershell
streamlit run llm/LLM_FE.py
```

### Má»›i
```powershell
streamlit run ui/app.py
```

## ğŸ”§ Code Migration

### 1. LLM Client Usage

#### CÅ© (Deprecated)
```python
from llm.LLM_API import call_gemini
from llm.LLM_LOCAL import call_lmstudio

# Call directly
response = call_gemini(messages)
response = call_lmstudio(messages)
```

#### Má»›i (OOP)
```python
from llm.client_factory import LLMClientFactory

# Create client via factory
client = LLMClientFactory.create_from_string("gemini")  # or "lmstudio"

# Generate response
response = client.generate(messages)
```

### 2. UI Components

#### CÅ© (Monolithic)
```python
# All UI logic trong LLM_FE.py (500+ lines)
# Sidebar, chat, sources Ä‘á»u trong 1 file
```

#### Má»›i (Modular)
```python
from ui.components import ChatDisplay, SourceDisplay, Sidebar

# Each component is a class
chat_display = ChatDisplay()
chat_display.render(messages)

sidebar = Sidebar(data_dir=Path("data/pdf"))
settings = sidebar.render()

source_display = SourceDisplay()
source_display.render(sources, retrieval_info)
```

### 3. Main App Structure

#### CÅ© (Procedural)
```python
# LLM_FE.py: Procedural code, top-to-bottom execution
st.sidebar...
st.markdown...
if prompt:
    # inline logic
```

#### Má»›i (OOP)
```python
# ui/app.py: OOP with clear responsibilities
class RAGChatApp:
    def __init__(self):
        self.chat_display = ChatDisplay()
        self.sidebar = Sidebar(...)
    
    def run(self):
        settings = self.sidebar.render()
        self.chat_display.render(messages)
        # ...
```

## ğŸ“ Benefits cá»§a cáº¥u trÃºc má»›i

### 1. Separation of Concerns
- **UI (ui/)**: Chá»‰ lo rendering
- **LLM (llm/)**: Chá»‰ lo gá»i LLM
- **Backend (pipeline/)**: Chá»‰ lo retrieval

### 2. OOP Design Patterns
- **Factory Pattern**: `LLMClientFactory` Ä‘á»ƒ táº¡o clients
- **Strategy Pattern**: `BaseLLMClient` â†’ swap providers dá»… dÃ ng
- **Component Pattern**: UI components reusable

### 3. Testability
```python
# Easy to test individual components
def test_gemini_client():
    client = GeminiClient(config={...})
    response = client.generate(test_messages)
    assert response == expected

def test_chat_display():
    display = ChatDisplay()
    # Test rendering logic
```

### 4. Maintainability
- Má»—i file < 300 lines
- Clear responsibilities
- Easy to locate bugs
- Easy to extend (thÃªm LLM provider má»›i)

## ğŸ”„ Backward Compatibility

### Old Files (Giá»¯ láº¡i táº¡m thá»i)
CÃ¡c file cÅ© váº«n Ä‘Æ°á»£c giá»¯ trong `llm/` Ä‘á»ƒ backward compatibility:
- `LLM_API.py` (deprecated)
- `LLM_LOCAL.py` (deprecated)
- `LLM_FE.py` (deprecated)

**LÆ°u Ã½**: Nhá»¯ng file nÃ y sáº½ bá»‹ xÃ³a trong version tiáº¿p theo.

### Migration Path
1. âœ… **Giai Ä‘oáº¡n 1** (Hiá»‡n táº¡i): Cáº¥u trÃºc má»›i Ä‘Æ°á»£c táº¡o song song
2. ğŸ”„ **Giai Ä‘oáº¡n 2** (Tiáº¿p theo): Update táº¥t cáº£ code sá»­ dá»¥ng file cÅ©
3. âŒ **Giai Ä‘oáº¡n 3** (Cuá»‘i cÃ¹ng): XÃ³a file cÅ©

## ğŸ§ª Testing New Structure

### Test UI
```powershell
streamlit run ui/app.py
```

### Test LLM Clients
```python
# Test trong Python REPL
from llm.client_factory import LLMClientFactory

# Test Gemini
gemini = LLMClientFactory.create_gemini()
print(gemini.is_available())
response = gemini.generate([{"role": "user", "content": "Hello"}])
print(response)

# Test LMStudio
lmstudio = LLMClientFactory.create_lmstudio()
print(lmstudio.is_available())
response = lmstudio.generate([{"role": "user", "content": "Hello"}])
print(response)
```

## ğŸ“š Documentation

- **UI Module**: `ui/README.md`
- **LLM Module**: `llm/README_NEW.md`
- **Components**: Inline docstrings trong má»—i class

## â“ FAQ

### Q: File cÅ© cÃ³ bá»‹ xÃ³a khÃ´ng?
A: ChÆ°a, giá»¯ láº¡i Ä‘á»ƒ backward compatibility. Sáº½ xÃ³a trong version sau.

### Q: CÃ³ cáº§n update code hiá»‡n cÃ³ khÃ´ng?
A: NÃªn update Ä‘á»ƒ sá»­ dá»¥ng cáº¥u trÃºc má»›i (OOP, modular). Old code váº«n cháº¡y nhÆ°ng deprecated.

### Q: CÃ¡ch thÃªm LLM provider má»›i?
A:
```python
# 1. Táº¡o class má»›i implement BaseLLMClient
class NewProviderClient(BaseLLMClient):
    def generate(self, messages, ...): ...
    def is_available(self): ...

# 2. ThÃªm vÃ o factory
class LLMClientFactory:
    @staticmethod
    def create_newprovider(...):
        return NewProviderClient(...)
```

### Q: UI cÃ³ thay Ä‘á»•i gÃ¬ khÃ´ng?
A: Giao diá»‡n giá»‘ng há»‡t, chá»‰ code architecture thay Ä‘á»•i.

## ğŸ¯ Next Steps

1. âœ… Cháº¡y thá»­ UI má»›i: `streamlit run ui/app.py`
2. âœ… Test cÃ¡c LLM clients
3. ğŸ”„ Update code cá»§a báº¡n Ä‘á»ƒ dÃ¹ng factory pattern
4. ğŸ“ Äá»c README trong `ui/` vÃ  `llm/` Ä‘á»ƒ hiá»ƒu rÃµ hÆ¡n

---

**Cáº­p nháº­t**: November 5, 2025
**Version**: 2.0.0
**Status**: âœ… Refactoring hoÃ n táº¥t, cáº¥u trÃºc má»›i Ä‘Ã£ sáºµn sÃ ng
