# RAG Evaluation Metrics Implementation Plan

## üéØ 3 Core Metrics: Ground-truth, Recall, Relevance

### üìã Current State Analysis
- **Ground-truth**: ‚úÖ ƒê√£ c√≥ s·∫µn trong database (`ground_truth_qa` table v·ªõi field `source`)
- **Semantic Similarity**: ‚úÖ ƒê√£ implement (ƒëo ƒë·ªô t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a)
- **Recall**: ‚ùå Ch∆∞a c√≥ - c·∫ßn implement
- **Relevance**: ‚ö†Ô∏è Partial - c√≥ semantic similarity nh∆∞ng c·∫ßn b·ªï sung

### üìù Implementation Todo List

#### 1. **Ground-truth Infrastructure** (T·∫≠n d·ª•ng existing)
- ‚úÖ Database table: `ground_truth_qa` v·ªõi fields: `question`, `answer`, `source`
- ‚úÖ Import functionality: Excel/CSV upload v·ªõi column mapping
- ‚úÖ UI component: GroundTruthComponent v·ªõi import v√† evaluation buttons
- üîÑ **C·∫ßn b·ªï sung**: Validation logic ƒë·ªÉ ƒë·∫£m b·∫£o source field c√≥ n·ªôi dung

#### 2. **Recall Metric** (Implement m·ªõi)
- **ƒê·ªãnh nghƒ©a**: T·ª∑ l·ªá chunks li√™n quan ƒë∆∞·ª£c t√¨m th·∫•y / t·ªïng s·ªë chunks li√™n quan
- **C√¥ng th·ª©c**: `Recall = (S·ªë chunks retrieved c√≥ semantic similarity > threshold) / (T·ªïng s·ªë chunks li√™n quan trong DB)`
- **C√°ch t√≠nh**:
  - L·∫•y t·∫•t c·∫£ chunks t·ª´ database c√≥ source t∆∞∆°ng t·ª± ground truth
  - So s√°nh v·ªõi retrieved chunks qua semantic similarity
  - ƒê·∫øm True Positives (chunks li√™n quan ƒë∆∞·ª£c t√¨m th·∫•y)
  - T√≠nh t·ª∑ l·ªá: TP / (TP + FN)

#### 3. **Relevance Metric** (T·∫≠n d·ª•ng + b·ªï sung semantic similarity)
- **ƒê·ªãnh nghƒ©a**: ƒê·ªô li√™n quan c·ªßa retrieved content v·ªõi query
- **T·∫≠n d·ª•ng**: Semantic similarity ƒë√£ c√≥ (cosine similarity v·ªõi source)
- **B·ªï sung**:
  - Query-chunk relevance (kh√¥ng ch·ªâ source-chunk)
  - Multi-level relevance scoring (0-1 scale)
  - Relevance threshold configuration

### üõ† Technical Implementation Plan

#### Phase 1: Enhance Backend API (`evaluation/backend_dashboard/api.py`)
1. **Th√™m method `evaluate_recall()`**:
   - Input: embedder_type, reranker_type, use_qem, top_k, threshold
   - Logic: So s√°nh retrieved chunks v·ªõi ground truth sources
   - Output: recall_score, precision, f1_score

2. **Th√™m method `evaluate_relevance()`**:
   - Input: embedder_type, reranker_type, use_qem, top_k
   - Logic: T√≠nh relevance score cho t·ª´ng retrieved chunk
   - Output: avg_relevance, relevance_distribution

3. **C·∫≠p nh·∫≠t `evaluate_ground_truth_with_semantic_similarity()`**:
   - Th√™m recall v√† relevance metrics v√†o k·∫øt qu·∫£

#### Phase 2: Update UI Component (`ui/dashboard/components/ground_truth.py`)
1. **Th√™m buttons m·ªõi**:
   - "üìà Evaluate Recall" - ch·∫°y recall evaluation
   - "üéØ Evaluate Relevance" - ch·∫°y relevance evaluation
   - "üìä Full Evaluation Suite" - ch·∫°y c·∫£ 3 metrics

2. **Th√™m display components**:
   - Recall metrics dashboard (precision, recall, F1)
   - Relevance score distribution charts
   - Comparative analysis across models

#### Phase 3: Database Schema Enhancement
1. **Th√™m metadata cho chunks**:
   - Chunk relevance scores
   - Source mapping cho recall calculation
   - Evaluation timestamps

#### Phase 4: Configuration & Thresholds
1. **Th√™m config file**: `evaluation_config.yaml`
   - Semantic similarity thresholds
   - Relevance scoring weights
   - Recall calculation parameters

### üìä Expected Output Metrics

#### Recall Evaluation:
```
{
  "recall_score": 0.75,        # 75% chunks li√™n quan ƒë∆∞·ª£c t√¨m th·∫•y
  "precision": 0.82,           # 82% retrieved chunks l√† li√™n quan
  "f1_score": 0.78,            # Harmonic mean
  "true_positives": 15,        # Chunks li√™n quan ƒë∆∞·ª£c t√¨m th·∫•y
  "false_positives": 3,        # Chunks kh√¥ng li√™n quan ƒë∆∞·ª£c retrieve
  "false_negatives": 5         # Chunks li√™n quan b·ªã b·ªè s√≥t
}
```

#### Relevance Evaluation:
```
{
  "avg_relevance": 0.68,       # Trung b√¨nh relevance score
  "high_relevance_ratio": 0.45, # T·ª∑ l·ªá chunks c√≥ relevance > 0.8
  "relevance_distribution": {
    "0-0.2": 5,
    "0.2-0.4": 12,
    "0.4-0.6": 18,
    "0.6-0.8": 22,
    "0.8-1.0": 13
  }
}
```

### üîÑ Integration Points
- **T·∫≠n d·ª•ng existing retrieval pipeline**: S·ª≠ d·ª•ng `retrieval_orchestrator.py`
- **T·∫≠n d·ª•ng existing embedders**: S·ª≠ d·ª•ng `embedder_factory.py`
- **T·∫≠n d·ª•ng existing ground truth**: S·ª≠ d·ª•ng `ground_truth_qa` table
- **T·∫≠n d·ª•ng existing UI framework**: Streamlit components

### üìà Success Criteria
1. **Ground-truth**: Import v√† validation ho·∫°t ƒë·ªông ho√†n h·∫£o
2. **Recall**: T√≠nh to√°n ch√≠nh x√°c v·ªõi ground truth sources
3. **Relevance**: Semantic similarity scores c√≥ √Ω nghƒ©a
4. **UI**: Dashboard hi·ªÉn th·ªã c·∫£ 3 metrics v·ªõi charts v√† comparisons
5. **Performance**: Evaluation ch·∫°y trong th·ªùi gian h·ª£p l√Ω (< 5 min cho 50 queries)

### üöÄ Next Steps
1. B·∫Øt ƒë·∫ßu v·ªõi Phase 1: Implement recall method trong backend API
2. Test v·ªõi sample data
3. Implement relevance enhancements
4. Update UI components
5. Full integration testing

---
**Priority**: Recall ‚Üí Relevance ‚Üí UI Enhancements</content>
<parameter name="filePath">d:\Project\RAG-2\RAG_EVALUATION_TODO.md