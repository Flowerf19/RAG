# RAG Pipeline - Retrieval-Augmented Generation System

[![Python Version](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

Há»‡ thá»‘ng RAG (Retrieval-Augmented Generation) modular, xá»­ lÃ½ PDF thÃ nh FAISS vector index vÃ  BM25 keyword index cho tÃ¬m kiáº¿m ngá»¯ nghÄ©a vÃ  keyword-based siÃªu nhanh. Há»— trá»£ multiple LLM providers (Ollama, OpenAI, Google Gemini) vá»›i giao diá»‡n Streamlit vÃ  reranking Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c.

## âœ¨ TÃ­nh nÄƒng chÃ­nh

- ğŸ” **Hybrid Retrieval**: Vector similarity (FAISS) + Keyword search (BM25) + Reranking
- ğŸ“„ **Advanced PDF Processing**: Text extraction, table parsing, OCR, multi-language support
- ğŸ§© **Modular Architecture**: Factory patterns, composition design, dependency injection
- ğŸ¤– **Multi-LLM Support**: Ollama, OpenAI, Google Gemini
- ğŸ§  **Multi-Embedder Support**: HuggingFace Local/API, Ollama Local
- ï¿½ **Query Enhancement**: QEM (Query Enhancement Module) cho cáº£i thiá»‡n truy váº¥n
- ï¿½ğŸ¨ **Modern UI**: Streamlit interface vá»›i chat vÃ  retrieval
- ğŸ“Š **Analytics**: Processing statistics, performance monitoring
- ğŸ”„ **Incremental Processing**: Cache-based Ä‘á»ƒ trÃ¡nh re-processing
- ğŸ”§ **Graceful Degradation**: Tá»± Ä‘á»™ng fallback khi services khÃ´ng kháº£ dá»¥ng

## ğŸš€ Khá»Ÿi Ä‘á»™ng nhanh

### YÃªu cáº§u há»‡ thá»‘ng

- **Python**: 3.10 (khuyáº¿n nghá»‹ - Ä‘Ã£ test vÃ  tÆ°Æ¡ng thÃ­ch vá»›i táº¥t cáº£ dependencies)
- **Ollama**: Server cháº¡y táº¡i `http://localhost:11434`
- **Models**: `embeddinggemma:latest`, `bge-m3:latest`
- **spaCy**: `en_core_web_sm`, `vi_core_news_lg`

### CÃ i Ä‘áº·t

```bash
# Clone repository
git clone https://github.com/Flowerf19/RAG.git
cd RAG

# Táº¡o virtual environment
python -m venv .venv
py -3.10 -m venv .venv  # Sá»­ dá»¥ng Python 3.10 cá»¥ thá»ƒ
.venv\Scripts\Activate.ps1  # Windows
# source .venv/bin/activate  # Linux/Mac

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt

# CÃ i Ä‘áº·t spaCy models
python -c "import spacy; spacy.cli.download('en_core_web_sm')"
python -c "import spacy; spacy.cli.download('vi_core_news_lg')"
```

### Khá»Ÿi Ä‘á»™ng Ollama

```bash
# CÃ i Ä‘áº·t vÃ  khá»Ÿi Ä‘á»™ng Ollama (náº¿u chÆ°a cÃ³)
# Download tá»«: https://ollama.ai/download

# Pull required models
ollama pull embeddinggemma:latest
ollama pull bge-m3:latest

# Verify models
ollama list
```

### Cháº¡y pipeline

```powershell
# Xá»­ lÃ½ táº¥t cáº£ PDF trong data/pdf/
python -c "from pipeline.rag_pipeline import RAGPipeline; RAGPipeline().process_directory('data/pdf')"

# Hoáº·c xá»­ lÃ½ file cá»¥ thá»ƒ
python -c "from pipeline.rag_pipeline import RAGPipeline; p = RAGPipeline(); p.process_pdf('path/to/file.pdf')"
```

### Cháº¡y giao diá»‡n web

```powershell
# Streamlit UI vá»›i tÃ­nh nÄƒng Embedding vÃ  Chat
streamlit run ui/app.py

# Truy cáº­p: http://localhost:8501
```

#### TÃ­nh nÄƒng Embedding trong UI

**ğŸ›ï¸ Äiá»u khiá»ƒn Embedding:**
- **NÃºt "ğŸš€ Run Embedding"**: Cháº¡y embedding cho táº¥t cáº£ PDF trong `data/pdf/`
- **Thanh tiáº¿n Ä‘á»™**: Hiá»ƒn thá»‹ tiáº¿n trÃ¬nh xá»­ lÃ½ tá»«ng file
- **NÃºt "â¹ï¸ Dá»«ng Embedding"**: Dá»«ng quÃ¡ trÃ¬nh embedding
- **Chuyá»ƒn Ä‘á»•i Embedder**: Chá»n giá»¯a HuggingFace Local/API hoáº·c Ollama

**ğŸ“Š Theo dÃµi tiáº¿n Ä‘á»™:**
- Sá»‘ file PDF Ä‘Æ°á»£c tÃ¬m tháº¥y
- File Ä‘ang xá»­ lÃ½ hiá»‡n táº¡i
- Pháº§n trÄƒm hoÃ n thÃ nh
- Tráº¡ng thÃ¡i chi tiáº¿t

### Cáº¥u hÃ¬nh Embedder

Há»‡ thá»‘ng há»— trá»£ multiple embedding providers:

- **HuggingFace Local**: Download vÃ  cháº¡y BGE-M3 1024-dim locally (default)
- **HuggingFace API**: Sá»­ dá»¥ng FREE Inference API vá»›i E5-Large Multilingual 1024-dim (cáº§n token)
- **Ollama Local**: Ollama server vá»›i embedding models (Gemma 768-dim, BGE-M3 1024-dim)

#### Model Specifications:

| Provider | Model | Dimensions | Max Tokens | Multilingual | Languages | Cost |
|----------|-------|------------|------------|--------------|-----------|------|
| HF Local | BAAI/bge-m3 | **1024** | 8192 | âœ… | 100+ | FREE |
| HF API | intfloat/multilingual-e5-large | **1024** | 512 | âœ… | **100+** | FREE |
| Ollama | embeddinggemma | 768 | 8192 | âœ… | 100+ | FREE |
| Ollama | bge-m3 | 1024 | 8192 | âœ… | 100+ | FREE |

**LÆ°u Ã½**: 
- HF API sá»­ dá»¥ng endpoint má»›i: `https://router.huggingface.co/hf-inference/` (migrated tá»« `api-inference.huggingface.co` - deprecated Nov 2025)
- **E5-Large Multilingual** há»— trá»£ 100+ ngÃ´n ngá»¯ bao gá»“m tiáº¿ng Viá»‡t, tiáº¿ng Anh, tiáº¿ng Trung, etc.
- **BGE-M3** (local) cÅ©ng há»— trá»£ Ä‘a ngÃ´n ngá»¯ tá»‘t, phÃ¹ há»£p khi khÃ´ng cÃ³ internet
- Cáº£ hai Ä‘á»u cho embeddings 1024 dimensions vÃ  hoÃ n toÃ n MIá»„N PHÃ

```bash
# HuggingFace API token (optional - cho HF API mode)
export HF_TOKEN="hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
# hoáº·c
export HUGGINGFACE_TOKEN="hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# VÃ­ dá»¥ setup trong Linux/Mac:
export HF_TOKEN="hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# Hoáº·c trong Windows PowerShell:
$env:HF_TOKEN="hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
```

**CÃ¡ch 2: Sá»­ dá»¥ng Streamlit secrets.toml** 
```toml
# File: .streamlit/secrets.toml
[huggingface]
api_token = "hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
# hoáº·c
hf_token = "hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
```

Trong UI, chá»n embedder phÃ¹ há»£p trong sidebar "Embedder source".

- **@Flowerf19** - Nguyá»…n HoÃ  (Hoaf.n.v@gmail.com) - Lead Developer
- **@lybachpha** - LeeWar (Bachien0987@gmail.com) - Core Contributor

## ğŸ“ Cáº¥u trÃºc project

```text
RAG/
â”œâ”€â”€ PDFLoaders/           # PDF processing vá»›i OCR integration
â”‚   â”œâ”€â”€ pdf_provider.py   # Smart PDF loading vá»›i PaddleOCR
â”‚   â””â”€â”€ pdf_extract_kit/  # Advanced PDF extraction toolkit
â”œâ”€â”€ chunkers/             # Semantic text chunking
â”‚   â”œâ”€â”€ semantic_chunker.py # spaCy-based chunking
â”‚   â””â”€â”€ model/            # Chunk data models
â”œâ”€â”€ embedders/            # Multi-provider embeddings
â”‚   â”œâ”€â”€ embedder_factory.py # Factory pattern cho embedders
â”‚   â””â”€â”€ providers/        # Ollama, HuggingFace implementations
â”œâ”€â”€ pipeline/             # Core RAG orchestration
â”‚   â”œâ”€â”€ rag_pipeline.py   # Main pipeline orchestrator
â”‚   â”œâ”€â”€ processing/       # PDF & embedding processing
â”‚   â”œâ”€â”€ retrieval/        # Hybrid retrieval + reranking
â”‚   â”œâ”€â”€ storage/          # FAISS, file management
â”‚   â””â”€â”€ backend_connector.py # Backward compatibility
â”œâ”€â”€ query_enhancement/    # Query Enhancement Module (QEM)
â”œâ”€â”€ reranking/            # Reranking cho improved accuracy
â”œâ”€â”€ BM25/                 # Keyword-based search (Whoosh)
â”œâ”€â”€ llm/                  # LLM integration (Ollama, Gemini, etc.)
â”œâ”€â”€ ui/                   # Streamlit UI vá»›i OOP components
â”œâ”€â”€ data/                 # Processed data vÃ  indexes
â”œâ”€â”€ config/               # Configuration files
â”œâ”€â”€ prompts/              # System prompts
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md             # Documentation
```

### Data Flow Architecture

```mermaid
graph TD
    A[PDF Files] --> B[PDFProvider]
    B --> C[SemanticChunker]
    C --> D[KeywordExtractor]
    C --> E[Embedder]
    D --> F[BM25 Index]
    E --> G[FAISS Index]

    H[User Query] --> I[Query Enhancement]
    I --> J[Embedder]
    J --> K[Hybrid Retrieval]
    K --> L{Retrieval Results}
    L --> M[Reranker]
    M --> N[Final Results]

    K -->|Vector Search| O[FAISS Search]
    K -->|Keyword Search| P[BM25 Search]
    O --> Q[Score Fusion]
    P --> Q
    Q --> L

    N --> R[Build Context]
    R --> S[LLM Generation]
    S --> T[Response]

    style A fill:#e1f5fe
    style F fill:#c8e6c9
    style G fill:#c8e6c9
    style T fill:#fff3e0
    style M fill:#ffebee
```

**Luá»“ng RAG hoáº¡t Ä‘á»™ng:**

1. **Indexing Phase**: PDF â†’ Chunks â†’ Embeddings â†’ FAISS/BM25 Indexes
2. **Query Enhancement Phase**: User Query â†’ QEM â†’ Enhanced Query
3. **Retrieval Phase**: Enhanced Query â†’ Hybrid Search (Vector + BM25) â†’ Score Fusion
4. **Reranking Phase**: Initial Results â†’ Reranker â†’ Improved Ranking
5. **Generation Phase**: Reranked Results + Query â†’ LLM â†’ Response

**Luá»“ng RAG hoáº¡t Ä‘á»™ng:**

1. **Indexing Phase**: PDF â†’ Chunks â†’ Embeddings â†’ FAISS/BM25 Indexes
2. **Query Enhancement Phase**: User Query â†’ QEM â†’ Enhanced Query
3. **Retrieval Phase**: Enhanced Query â†’ Hybrid Search (Vector + BM25) â†’ Score Fusion
4. **Reranking Phase**: Initial Results â†’ Reranker â†’ Improved Ranking
5. **Generation Phase**: Reranked Results + Query â†’ LLM â†’ Response

### Reranking Support

Há»‡ thá»‘ng há»— trá»£ multiple reranking providers Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c cá»§a káº¿t quáº£ tÃ¬m kiáº¿m:

- **BGE-M3 Local**: `BAAI/bge-reranker-v2-m3` (free, high performance)
- **BGE-M3 Ollama**: Ollama-based reranking (no additional dependencies)
- **HuggingFace API**: Cloud-based reranking vá»›i API tokens

**Cáº¥u hÃ¬nh Reranking:**
```python
# Trong retrieval orchestrator
reranker = RerankerFactory.create_bge_m3_hf_local(device="cpu")
results = reranker.rerank(query, candidates, top_k=5)
```

## ğŸ”§ Sá»­ dá»¥ng trong code

### Basic Pipeline Usage

```python
from pipeline.rag_pipeline import RAGPipeline
from pathlib import Path

# Initialize pipeline
pipeline = RAGPipeline()

# Process PDF directory
pdf_dir = Path("data/pdf")
pipeline.process_directory(pdf_dir)

# Process single PDF
pdf_path = Path("data/pdf/document.pdf")
pipeline.process_pdf(pdf_path)
```

### Advanced Retrieval vá»›i Reranking

```python
from pipeline.retrieval.retrieval_orchestrator import fetch_retrieval

# Enhanced retrieval vá»›i query enhancement vÃ  reranking
results = fetch_retrieval(
    query_text="machine learning algorithms",
    top_k=5,
    embedder_type="ollama",
    reranker_type="bge_m3_hf_local",
    use_query_enhancement=True
)

print(f"Context: {results['context'][:200]}...")
print(f"Sources: {len(results['sources'])} documents")
```

### LLM Integration

```python
from llm.client_factory import LLMClientFactory
from llm import LLMProvider

# Create LLM client
client = LLMClientFactory.create(LLMProvider.GEMINI, config={"api_key": "your-key"})
response = client.generate_response("Explain RAG systems")

# Hoáº·c sá»­ dá»¥ng Ollama
from llm import LLMLocal
local_llm = LLMLocal()
response = local_llm.generate_response("Explain RAG systems")
```

### Custom Chunking vá»›i Aggregation

```python
from chunkers.semantic_chunker import SemanticChunker

# Configure chunker vá»›i multi-language support
chunker = SemanticChunker(
    max_tokens=200,
    overlap_tokens=20,
    language="vi"  # Support: en, vi, zh, fr, de, es, etc.
)

# Process document (tá»± Ä‘á»™ng aggregate text + tables + figures)
chunk_set = chunker.chunk(pdf_document)
```

## ğŸ§ª Testing

```powershell
# Run integration test
python test_kit_integration.py

# Test specific components
python -c "from PDFLoaders.pdf_provider import PDFProvider; p = PDFProvider(); print('PDF Provider OK')"

# Test embedding
python -c "from embedders.embedder_factory import EmbedderFactory; emb = EmbedderFactory.create('ollama', {}); print('Embedder OK')"
```

## ğŸ“Š Performance & Monitoring

### Benchmark Results

- **PDF Processing**: ~50 pages/minute (vá»›i OCR enhancement)
- **Vector Search**: < 10ms cho 10K documents
- **BM25 Search**: < 5ms cho keyword queries
- **Reranking**: < 100ms cho 20 candidates (BGE-M3 local)
- **Memory Usage**: ~2GB cho 1K documents
- **Query Enhancement**: < 50ms per query (QEM module)

### Monitoring Commands

```powershell
# Check FAISS index integrity
python -c "from pipeline.storage.vector_store import VectorStore; store = VectorStore(); index = store.load_index('data/vectors/doc.faiss'); print(f'Vectors: {index.ntotal}')"

# View processing statistics
python -c "import json; data = json.load(open('data/batch_summary_*.json')); print(f'Processed: {data[\"total_files\"]} files')"

# Check BM25 index
python -c "from BM25.bm25_manager import BM25Manager; bm25 = BM25Manager(); print(f'BM25 docs: {bm25.get_doc_count()}')"
```

## ğŸ”§ Configuration

### Environment Variables

```bash
# LLM Configuration
export OPENAI_API_KEY="your-key"
export GOOGLE_API_KEY="your-key"
export OLLAMA_BASE_URL="http://localhost:11434"

# System Configuration
export RAG_DATA_DIR="./data"
export RAG_CACHE_DIR="./data/cache"
export RAG_LOG_LEVEL="INFO"
```

### YAML Configuration (`config/app.yaml`)

```yaml
llm:
  default_provider: "ollama"
  models:
    embedding: "embeddinggemma:latest"
    generation: "llama2:7b"

processing:
  chunk_size: 200
  overlap: 20
  batch_size: 32

search:
  vector_top_k: 5
  bm25_top_k: 5
  hybrid_weight_vector: 0.7
  hybrid_weight_bm25: 0.3

reranking:
  default_provider: "bge_m3_hf_local"
  top_k: 5
  device: "cpu"
```

## ğŸš¨ Troubleshooting

### Common Issues

#### Ollama Connection Failed

```bash
# Check Ollama status
curl http://localhost:11434/api/tags

# Restart Ollama service
ollama serve
```

#### PDF Processing Errors

```powershell
# Clear cache and re-process
Remove-Item "data\cache\*.json" -Force
python -c "from pipeline.rag_pipeline import RAGPipeline; RAGPipeline().process_directory('data/pdf')"
```

#### Embedding Dimension Mismatch

```powershell
# Khi chuyá»ƒn Ä‘á»•i embedder, rebuild indexes
Remove-Item "data\vectors\*" -Force
Remove-Item "data\embeddings\*" -Force
# Rerun embedding process
```

#### Memory Issues

```powershell
# Reduce batch size in config
# Use smaller embedding model
ollama pull embeddinggemma:latest  # Instead of bge-m3
```

#### Streamlit Errors

```powershell
# Clear Streamlit cache
streamlit cache clear
streamlit run ui/app.py --server.port 8501
```

### Debug Tools

```powershell
# Debug PDF processing
python -c "from PDFLoaders.pdf_provider import PDFProvider; p = PDFProvider(); doc = p.load_pdf('test.pdf'); print(f'Pages: {len(doc.pages)}')"

# Debug chunking
python -c "from chunkers.semantic_chunker import SemanticChunker; c = SemanticChunker(); print('Chunker OK')"

# Debug embeddings
python -c "from embedders.embedder_factory import EmbedderFactory; emb = EmbedderFactory.create('ollama', {}); vec = emb.embed_text('test'); print(f'Dim: {len(vec)}')"

# Debug LLM
python -c "from llm.client_factory import LLMClientFactory; client = LLMClientFactory.create_gemini({}); print('LLM OK')"
```

## ğŸ¤ Contributing

### Development Setup

```bash
# Fork and clone
git clone https://github.com/Flowerf19/RAG.git
cd RAG

# Create feature branch
git checkout -b feature/new-feature

# Install dev dependencies (if available)
pip install -r requirements-dev.txt

# Run tests before committing
python test_kit_integration.py
```

### Code Standards

- **Language**: Vietnamese comments, English docstrings
- **Style**: Black formatter, isort imports
- **Testing**: pytest vá»›i coverage > 80%
- **Documentation**: Update README cho breaking changes

### Architecture Guidelines

- **Composition over Inheritance**: Use composition patterns
- **Factory Methods**: For complex object creation
- **Protocol Interfaces**: For testability
- **Error Handling**: Graceful degradation

## ğŸ“ˆ Roadmap

### Phase 1 (Current) âœ…

- âœ… Advanced PDF processing vá»›i OCR integration
- âœ… Hybrid retrieval (FAISS + BM25)
- âœ… Query enhancement module (QEM)
- âœ… Reranking support (multiple providers)
- âœ… Multi-LLM support (Ollama, Gemini, OpenAI)
- âœ… Modular architecture vá»›i factory patterns
- âœ… Streamlit UI vá»›i chat vÃ  embedding controls
- âœ… Multi-language support (spaCy models)

### Phase 2 (Future) ğŸ”„

- ğŸ”„ Incremental processing vÃ  cache optimization
- ğŸ”„ Cloud storage support (AWS S3, Google Cloud)
- ğŸ”„ Advanced chunking strategies (recursive, semantic)
- ğŸ”„ Multi-modal support (images, diagrams)
- ğŸ”„ API endpoints cho web integration
- ğŸ”„ Performance monitoring vÃ  analytics dashboard
- ğŸ”„ SELF RAG implementation
- ğŸ”„ Advanced caching system
- ğŸ”„ GRAPRAG module integration
- ğŸ”„ REST API development
- ğŸ”„ Real-time indexing vÃ  streaming

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **FAISS**: Facebook AI Similarity Search
- **Ollama**: Local LLM inference
- **spaCy**: Industrial-strength NLP
- **Whoosh**: Pure Python search engine
- **Streamlit**: Fast web apps for ML
- **PaddleOCR**: Advanced OCR cho multi-language PDFs
- **HuggingFace**: Transformers vÃ  embedding models
- **BAAI**: BGE models cho embeddings vÃ  reranking

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/Flowerf19/RAG/issues)
- **Discussions**: [GitHub Discussions](https://github.com/Flowerf19/RAG/discussions)
- **Documentation**: See module READMEs for technical details
- **Email**: Hoaf.n.v@gmail.com | Bachien0987@gmail.com
