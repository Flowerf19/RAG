# RAG Pipeline - Retrieval-Augmented Generation System

[![Python Version](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

Há»‡ thá»‘ng RAG (Retrieval-Augmented Generation) modular, xá»­ lÃ½ PDF thÃ nh FAISS vector index vÃ  BM25 keyword index cho tÃ¬m kiáº¿m ngá»¯ nghÄ©a vÃ  keyword-based siÃªu nhanh. Há»— trá»£ multiple LLM providers (Ollama, OpenAI, Google Gemini) vá»›i giao diá»‡n Streamlit.

## âœ¨ TÃ­nh nÄƒng chÃ­nh

- ğŸ” **Dual Retrieval**: Vector similarity (FAISS) + Keyword search (BM25)
- ğŸ“„ **PDF Processing**: Text extraction, table parsing, multi-language support
- ğŸ§© **Modular Architecture**: Factory patterns, composition design
- ğŸ¤– **Multi-LLM Support**: Ollama, OpenAI, Google Gemini
- ğŸ§  **Multi-Embedder Support**: HuggingFace Local/API, Ollama Local
- ğŸ¨ **Modern UI**: Streamlit interface vá»›i chat vÃ  retrieval
- ğŸ“Š **Analytics**: Processing statistics, performance monitoring
- ğŸ”„ **Incremental Processing**: Cache-based Ä‘á»ƒ trÃ¡nh re-processing

## ğŸš€ Khá»Ÿi Ä‘á»™ng nhanh

### YÃªu cáº§u há»‡ thá»‘ng

- **Python**: >= 3.13
- **Ollama**: Server cháº¡y táº¡i `http://localhost:11434`
- **Models**: `embeddinggemma:latest`, `bge-m3:latest`
- **spaCy**: `en_core_web_sm`, `vi_core_news_lg`

### CÃ i Ä‘áº·t

```bash
# Clone repository
git clone <repository-url>
cd RAG

# Táº¡o virtual environment
python -m venv .venv
py -3 -m venv .venv
.venv\Scripts\Activate.ps1  # Windows
# source .venv/bin/activate  # Linux/Mac

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt

# CÃ i Ä‘áº·t spaCy models
python -c "import spacy; spacy.cli.download('en_core_web_sm')"
python -c "import spacy; spacy.cli.download('vi_core_news_lg')"
```

### Khá»Ÿi Ä‘á»™ng Ollama

```bash
# CÃ i Ä‘áº·t vÃ  khá»Ÿi Ä‘á»™ng Ollama (náº¿u chÆ°a cÃ³)
# Download tá»«: https://ollama.ai/download

# Pull required models
ollama pull embeddinggemma:latest
ollama pull bge-m3:latest

# Verify models
ollama list
```

### Cháº¡y pipeline

```powershell
# Xá»­ lÃ½ táº¥t cáº£ PDF trong data/pdf/
python run_pipeline.py

# Hoáº·c xá»­ lÃ½ file cá»¥ thá»ƒ
python -c "from pipeline import RAGPipeline; p = RAGPipeline(); p.process_pdf('path/to/file.pdf')"
```

### Cháº¡y giao diá»‡n web

```powershell
# Streamlit UI vá»›i tÃ­nh nÄƒng Embedding
streamlit run llm/LLM_FE.py

# Truy cáº­p: http://localhost:8501
```

#### TÃ­nh nÄƒng Embedding trong UI

**ğŸ›ï¸ Äiá»u khiá»ƒn Embedding:**
- **NÃºt "ğŸš€ Run Embedding"**: Cháº¡y embedding cho táº¥t cáº£ PDF trong `data/pdf/`
- **Thanh tiáº¿n Ä‘á»™**: Hiá»ƒn thá»‹ tiáº¿n trÃ¬nh xá»­ lÃ½ tá»«ng file
- **NÃºt "â¹ï¸ Dá»«ng Embedding"**: Dá»«ng quÃ¡ trÃ¬nh embedding
- **Chuyá»ƒn Ä‘á»•i Embedder**: Chá»n giá»¯a HuggingFace Local/API hoáº·c Ollama

**ğŸ“Š Theo dÃµi tiáº¿n Ä‘á»™:**
- Sá»‘ file PDF Ä‘Æ°á»£c tÃ¬m tháº¥y
- File Ä‘ang xá»­ lÃ½ hiá»‡n táº¡i
- Pháº§n trÄƒm hoÃ n thÃ nh
- Tráº¡ng thÃ¡i chi tiáº¿t

### Cáº¥u hÃ¬nh Embedder

Há»‡ thá»‘ng há»— trá»£ multiple embedding providers:

- **HuggingFace Local**: Download vÃ  cháº¡y BGE-M3 1024-dim locally (default)
- **HuggingFace API**: Sá»­ dá»¥ng FREE Inference API vá»›i E5-Large Multilingual 1024-dim (cáº§n token)
- **Ollama Local**: Ollama server vá»›i embedding models (Gemma 768-dim, BGE-M3 1024-dim)

#### Model Specifications:

| Provider | Model | Dimensions | Max Tokens | Multilingual | Languages | Cost |
|----------|-------|------------|------------|--------------|-----------|------|
| HF Local | BAAI/bge-m3 | **1024** | 8192 | âœ… | 100+ | FREE |
| HF API | intfloat/multilingual-e5-large | **1024** | 512 | âœ… | **100+** | FREE |
| Ollama | embeddinggemma | 768 | 8192 | âœ… | 100+ | FREE |
| Ollama | bge-m3 | 1024 | 8192 | âœ… | 100+ | FREE |

**LÆ°u Ã½**: 
- HF API sá»­ dá»¥ng endpoint má»›i: `https://router.huggingface.co/hf-inference/` (migrated tá»« `api-inference.huggingface.co` - deprecated Nov 2025)
- **E5-Large Multilingual** há»— trá»£ 100+ ngÃ´n ngá»¯ bao gá»“m tiáº¿ng Viá»‡t, tiáº¿ng Anh, tiáº¿ng Trung, etc.
- **BGE-M3** (local) cÅ©ng há»— trá»£ Ä‘a ngÃ´n ngá»¯ tá»‘t, phÃ¹ há»£p khi khÃ´ng cÃ³ internet
- Cáº£ hai Ä‘á»u cho embeddings 1024 dimensions vÃ  hoÃ n toÃ n MIá»„N PHÃ

```bash
# HuggingFace API token (optional - cho HF API mode)
export HF_TOKEN="hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
# hoáº·c
export HUGGINGFACE_TOKEN="hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# VÃ­ dá»¥ setup trong Linux/Mac:
export HF_TOKEN="hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# Hoáº·c trong Windows PowerShell:
$env:HF_TOKEN="hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
```

**CÃ¡ch 2: Sá»­ dá»¥ng Streamlit secrets.toml** (khuyáº¿n nghá»‹ cho development)
```toml
# File: .streamlit/secrets.toml
[huggingface]
api_token = "hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
# hoáº·c
hf_token = "hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
```

**CÃ¡ch láº¥y HuggingFace Token (MIá»„N PHÃ):**
1. Truy cáº­p [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
2. ÄÄƒng nháº­p tÃ i khoáº£n HuggingFace (hoáº·c Ä‘Äƒng kÃ½ miá»…n phÃ­)
3. Táº¡o "New token" vá»›i type "Read"
4. Copy token vÃ  thiáº¿t láº­p nhÆ° hÆ°á»›ng dáº«n trÃªn

**LÆ°u Ã½**: HuggingFace Inference API hoÃ n toÃ n MIá»„N PHÃ cho BGE-M3 model!

Trong UI, chá»n embedder phÃ¹ há»£p trong sidebar "Embedder source".

## ğŸ“ Cáº¥u trÃºc project

```text
RAG/
â”œâ”€â”€ pipeline/          # Core orchestration, FAISS, retriever
â”œâ”€â”€ loaders/           # PDF processing, text/table extraction
â”œâ”€â”€ chunkers/          # Text chunking strategies
â”œâ”€â”€ embedders/         # Ollama embedding providers
â”œâ”€â”€ llm/              # LLM integration (API, local, UI)
â”œâ”€â”€ BM25/             # Keyword-based search (Whoosh)
â”œâ”€â”€ data/             # Processed data vÃ  indexes
â”œâ”€â”€ config/           # Configuration files
â”œâ”€â”€ prompts/          # System prompts
â”œâ”€â”€ tests/            # Unit vÃ  integration tests
â”œâ”€â”€ requirements.txt  # Python dependencies
â”œâ”€â”€ run_pipeline.py   # Main entry point
â””â”€â”€ README.md         # Documentation
```

### Data Flow Architecture

```mermaid
graph TD
    A[PDF Files] --> B[PDFLoader]
    B --> C[HybridChunker]
    C --> D[KeywordExtractor]
    C --> E[OllamaEmbedder]
    D --> F[BM25Indexer]
    E --> G[FAISS Index]
    F --> H[Whoosh Index]

    I[User Query] --> J[Retriever]
    J --> K{Search Type}
    K -->|Vector| L[FAISS Search]
    K -->|Keyword| M[BM25 Search]
    K -->|Hybrid| N[Multi Search]

    L --> O[Relevant Chunks]
    M --> O
    N --> O

    O --> P[Build Context]
    P --> Q[LLM with Context]
    Q --> R[Generate Response]

    style A fill:#e1f5fe
    style G fill:#c8e6c9
    style H fill:#c8e6c9
    style R fill:#fff3e0
```

**Luá»“ng RAG hoáº¡t Ä‘á»™ng:**

1. **Indexing Phase**: PDF â†’ Chunks â†’ Embeddings â†’ FAISS/BM25 Indexes
2. **Retrieval Phase**: Query â†’ Search Indexes â†’ Relevant Chunks
3. **Generation Phase**: Query + Relevant Chunks â†’ LLM â†’ Response

## ğŸ”§ Sá»­ dá»¥ng trong code

### Basic Pipeline Usage

```python
from pipeline import RAGPipeline
from pathlib import Path

# Initialize pipeline
pipeline = RAGPipeline()

# Process PDF
pdf_path = Path("data/pdf/document.pdf")
pipeline.process_pdf(pdf_path)

# Search with vector similarity
results = pipeline.search_similar(
    faiss_file=Path("data/vectors/document_vectors_20251021.faiss"),
    metadata_map_file=Path("data/vectors/document_metadata_map_20251021.pkl"),
    query_text="machine learning algorithms",
    top_k=5
)

# Search with BM25
bm25_results = pipeline.search_bm25("query text", top_k=5)

# Combined search
hybrid_results = pipeline.hybrid_search("query", vector_weight=0.7, bm25_weight=0.3)
```

### LLM Integration

```python
from llm import LLMAPI, LLMLocal

# OpenAI/Gemini API
api_llm = LLMAPI()
response = api_llm.generate_response("Explain RAG systems")

# Local Ollama
local_llm = LLMLocal()
response = local_llm.generate_response("Explain RAG systems")
```

### Custom Chunking

```python
from chunkers import HybridChunker
from chunkers.model import ChunkerMode

# Configure chunker
chunker = HybridChunker(
    max_tokens=200,
    overlap_tokens=20,
    mode=ChunkerMode.AUTO
)

# Process document
chunk_set = chunker.chunk(pdf_document)
```

## ğŸ§ª Testing

```powershell
# Run all tests
python -m pytest -v --cov=.

# Test specific modules
python -m pytest tests/pipeline/ -v
python -m pytest tests/loaders/ -v
python -m pytest tests/chunkers/ -v

# Integration tests
python -m pytest tests/e2e/ -v
```

## ğŸ“Š Performance & Monitoring

### Benchmark Results

- **PDF Processing**: ~50 pages/minute
- **Vector Search**: < 10ms cho 10K documents
- **BM25 Search**: < 5ms cho keyword queries
- **Memory Usage**: ~2GB cho 1K documents

### Monitoring Commands

```powershell
# Check FAISS index integrity
python -c "from pipeline.vector_store import VectorStore; store = VectorStore(); index = store.load_index('data/vectors/doc.faiss'); print(f'Vectors: {index.ntotal}')"

# View processing statistics
python -c "import json; data = json.load(open('data/batch_summary_*.json')); print(f'Processed: {data[\"total_files\"]} files')"
```

## ğŸ”§ Configuration

### Environment Variables

```bash
# LLM Configuration
export OPENAI_API_KEY="your-key"
export GOOGLE_API_KEY="your-key"
export OLLAMA_BASE_URL="http://localhost:11434"

# System Configuration
export RAG_DATA_DIR="./data"
export RAG_CACHE_DIR="./data/cache"
export RAG_LOG_LEVEL="INFO"
```

### YAML Configuration (`config/app.yaml`)

```yaml
llm:
  default_provider: "ollama"
  models:
    embedding: "embeddinggemma:latest"
    generation: "llama2:7b"

processing:
  chunk_size: 200
  overlap: 20
  batch_size: 32

search:
  vector_top_k: 5
  bm25_top_k: 5
  hybrid_weight_vector: 0.7
  hybrid_weight_bm25: 0.3
```

## ğŸš¨ Troubleshooting

### Common Issues

#### Ollama Connection Failed

```bash
# Check Ollama status
curl http://localhost:11434/api/tags

# Restart Ollama service
ollama serve
```

#### PDF Processing Errors

```powershell
# Clear cache and re-process
Remove-Item "data\cache\*.json" -Force
python run_pipeline.py
```

#### Memory Issues

```powershell
# Reduce batch size in config
# Use smaller embedding model
ollama pull embeddinggemma:latest  # Instead of bge-m3
```

#### Streamlit Errors

```powershell
# Clear Streamlit cache
streamlit cache clear
streamlit run llm/LLM_FE.py --server.port 8501
```

### Debug Tools

```powershell
# Debug chunking
python chunkers/chunk_pdf_demo.py

# Debug embeddings
python -c "from embedders import OllamaEmbedder; emb = OllamaEmbedder(); print(emb.embed_text('test'))"

# Debug LLM
python -c "from llm import LLMLocal; llm = LLMLocal(); print(llm.generate_response('Hello'))"
```

## ğŸ¤ Contributing

### Development Setup

```bash
# Fork and clone
git clone https://github.com/your-username/RAG.git
cd RAG

# Create feature branch
git checkout -b feature/new-feature

# Install dev dependencies
pip install -r requirements-dev.txt

# Run tests before committing
python -m pytest
```

### Code Standards

- **Language**: Vietnamese comments, English docstrings
- **Style**: Black formatter, isort imports
- **Testing**: pytest vá»›i coverage > 80%
- **Documentation**: Update README cho breaking changes

### Architecture Guidelines

- **Composition over Inheritance**: Use composition patterns
- **Factory Methods**: For complex object creation
- **Protocol Interfaces**: For testability
- **Error Handling**: Graceful degradation

## ğŸ“ˆ Roadmap

### Phase 1 (Current)

- âœ… PDF processing pipeline
- âœ… FAISS vector search
- âœ… BM25 keyword search
- âœ… Multi-LLM support
- âœ… Streamlit UI

### Phase 2 (Next)

- ğŸ”„ Incremental processing
- ğŸ”„ Cloud storage support
- ğŸ”„ Advanced chunking strategies
- ğŸ”„ Query expansion
- ğŸ”„ Multi-modal support

### Phase 3 (Future)

- ğŸ”„ Distributed processing
- ğŸ”„ Real-time indexing
- ğŸ”„ Advanced analytics
- ğŸ”„ Plugin architecture
- ğŸ”„ Web API

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **FAISS**: Facebook AI Similarity Search
- **Ollama**: Local LLM inference
- **spaCy**: Industrial-strength NLP
- **Whoosh**: Pure Python search engine
- **Streamlit**: Fast web apps for ML

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/your-username/RAG/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-username/RAG/discussions)
- **Documentation**: See module READMEs for technical details
