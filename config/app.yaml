ui:
  default_backend: gemini  # gemini | lmstudio

paths:
  data_dir: data
  prompt_path: prompt/system_prompt.txt

llm:
  gemini:
    model: gemini-2.0-flash
    temperature: null  # use provider default when null
    max_tokens: null   # use provider default when null
    api_key:
      secrets_key: gemini_api_key
      env: GEMINI_API_KEY

  lmstudio:
    base_url: http://127.0.0.1:1234/v1
    api_key:
      env: LMSTUDIO_API_KEY
      default: lm-studio  # local default; override in env for prod
    model: google/gemma-3-4b
    sampling:
      temperature: 0.
      top_p: 0.9
      max_tokens: 512


