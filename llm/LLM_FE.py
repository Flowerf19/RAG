import sys
import os
from pathlib import Path
import streamlit as st
import shutil
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))
sys.path.append(os.path.dirname(__file__))  # Add current directory


# Import chat_handler v√† LLM clients
# Handle both direct execution and module import
try:
    # When run as module
    from .chat_handler import build_messages
    from .LLM_API import call_gemini
    from .LLM_LOCAL import call_lmstudio
    from .config_loader import ui_default_backend, paths_data_dir
except ImportError:
    # When run directly as script
    from chat_handler import build_messages
    from LLM_API import call_gemini
    from LLM_LOCAL import call_lmstudio
    from config_loader import ui_default_backend, paths_data_dir

# Import pipeline_qa
try:
    from pipeline.backend_connector import fetch_retrieval
except ImportError:
    try:
        sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
        from pipeline.backend_connector import fetch_retrieval
    except ImportError:
        raise


# === PAGE CONFIG ===
st.set_page_config(page_title="AI Chatbot", page_icon=":speech_balloon:", layout="wide")

# === GLOBAL STYLES ===
css_path = Path(__file__).with_name("chat_styles.css")
st.markdown(f"<style>{css_path.read_text(encoding='utf-8')}</style>", unsafe_allow_html=True)

# === SIDEBAR ===
with st.sidebar:
    st.markdown("### Menu")
    
    # New Chat button
    if st.button("New Chat"):
        st.session_state["messages"] = []
        st.session_state["is_generating"] = False
        st.session_state["pending_prompt"] = None
        st.session_state["last_sources"] = []
        st.rerun()
    
    # Clear Cache button
    if st.button("Clear Cache"):
        st.session_state.clear()
        st.rerun()

    st.markdown("---")
    
    # === UPLOAD FILE ===
    st.markdown("### Upload file")
    uploaded_file = st.file_uploader("Ch·ªçn file ƒë·ªÉ t·∫£i l√™n", type=["pdf", "docx", "txt"])

    if uploaded_file is not None:
        save_dir_path = paths_data_dir()
        os.makedirs(save_dir_path, exist_ok=True)
        save_path = os.path.join(str(save_dir_path), uploaded_file.name)

        # L∆∞u file v·ªÅ th∆∞ m·ª•c data
        with open(save_path, "wb") as f:
            shutil.copyfileobj(uploaded_file, f)

        st.success(f"ƒê√£ l∆∞u file: {uploaded_file.name}")

    st.markdown("---")
    st.markdown("<div style='flex: 1;'></div>", unsafe_allow_html=True)  # Spacer

    # === BACKEND SELECTION ===
    backend_options = ["gemini", "lmstudio"]
    if "backend_mode" not in st.session_state:
        default_backend = ui_default_backend()
        st.session_state["backend_mode"] = (
            default_backend if default_backend in backend_options else backend_options[0]
        )

    st.markdown("<div class='sidebar-footer'>", unsafe_allow_html=True)
    st.radio(
        "Response source",
        backend_options,
        key="backend_mode",
        help="Ch·ªçn ngu·ªìn tr·∫£ l·ªùi cho chatbot",
        format_func=lambda x: "Gemini API" if x == "gemini" else "LM Studio Local"
    )
    
    # Embedding Model Selection
    st.markdown("---")
    embedding_options = ["ollama", "huggingface_local", "huggingface_api"]
    if "embedder_type" not in st.session_state:
        st.session_state["embedder_type"] = "huggingface_local"  # Default to BGE-M3 local
    
    st.radio(
        "Embedding Model",
        embedding_options,
        key="embedder_type",
        help="Ch·ªçn lo·∫°i embedder cho retrieval",
        format_func=lambda x: {
            "ollama": "Ollama (Gemma/BGE-M3)",
            "huggingface_local": "HF Local (BGE-M3 1024-dim)",
            "huggingface_api": "HF API (E5-Large 1024-dim)"
        }.get(x, x)
    )
    
    # Reranker Model Selection
    st.markdown("---")
    reranker_options = ["none", "bge_local", "bge_m3_ollama", "bge_m3_hf_local", "bge_m3_hf_api"]
    if "reranker_type" not in st.session_state:
        st.session_state["reranker_type"] = "bge_m3_hf_local"  # Default to BGE-M3 HF local
    
    st.radio(
        "Reranker Model",
        reranker_options,
        key="reranker_type",
        help="Ch·ªçn lo·∫°i reranker ƒë·ªÉ s·∫Øp x·∫øp l·∫°i k·∫øt qu·∫£",
        format_func=lambda x: {
            "none": "No Reranking",
            "bge_local": "BGE v2-m3 Local",
            "bge_m3_ollama": "BGE-M3 Ollama",
            "bge_m3_hf_local": "BGE-M3 HF Local",
            "bge_m3_hf_api": "Sentence-Transformers HF API",
            "cohere": "Cohere API",
            "jina": "Jina API"
        }.get(x, x)
    )
    
    # Top K Settings
    st.markdown("---")
    st.markdown("### Retrieval Settings")
    
    # Top K for Embedding Retrieval
    if "top_k_embed" not in st.session_state:
        st.session_state["top_k_embed"] = 10
    
    st.slider(
        "Top K Embedding Retrieval",
        min_value=5,
        max_value=50,
        value=st.session_state.get("top_k_embed", 10),
        step=5,
        key="top_k_embed",
        help="S·ªë l∆∞·ª£ng k·∫øt qu·∫£ t·ª´ embedding search (tr∆∞·ªõc reranking)"
    )
    
    # Top K for Reranking
    if "top_k_rerank" not in st.session_state:
        st.session_state["top_k_rerank"] = 5
    
    st.slider(
        "Top K Reranking",
        min_value=1,
        max_value=20,
        value=st.session_state.get("top_k_rerank", 5),
        step=1,
        key="top_k_rerank",
        help="S·ªë l∆∞·ª£ng k·∫øt qu·∫£ cu·ªëi c√πng sau reranking"
    )

    # Query Enhancement Toggle
    st.markdown("---")
    if "use_query_enhancement" not in st.session_state:
        st.session_state["use_query_enhancement"] = True  # Default to enabled

    st.checkbox(
        "üîç Query Enhancement (QEM)",
        value=st.session_state.get("use_query_enhancement", True),
        key="use_query_enhancement",
        help="T·ª± ƒë·ªông m·ªü r·ªông query ƒë·ªÉ c·∫£i thi·ªán k·∫øt qu·∫£ t√¨m ki·∫øm (v√≠ d·ª•: 'qu·∫£n l√Ω r·ªßi ro' ‚Üí 'qu·∫£n tr·ªã r·ªßi ro', 'ki·ªÉm so√°t r·ªßi ro', ...)"
    )

    
    # API token status for API-based rerankers
    reranker_type = st.session_state.get("reranker_type", "bge_m3_hf_local")
    if reranker_type in ["bge_m3_hf_api", "cohere", "jina"]:
        try:
            if reranker_type == "bge_m3_hf_api":
                from embedders.providers.huggingface.token_manager import get_hf_token
                token = get_hf_token()
                service_name = "HuggingFace"
            elif reranker_type == "cohere":
                token = os.getenv("COHERE_API_KEY") or os.getenv("COHERE_TOKEN")
                service_name = "Cohere"
            elif reranker_type == "jina":
                token = os.getenv("JINA_API_KEY") or os.getenv("JINA_TOKEN")
                service_name = "Jina"
            
            if token:
                st.success(f"‚úÖ {service_name} API token: OK")
            else:
                st.warning(f"‚ö†Ô∏è {service_name} token ch∆∞a thi·∫øt l·∫≠p")
        except Exception as e:
            st.error(f"‚ö†Ô∏è L·ªói token: {e}")
    
    # === EMBEDDING CONTROLS ===
    st.markdown("### Embedding Controls")
    
    # Show PDF count
    pdf_dir = Path("data/pdf")
    if pdf_dir.exists():
        pdf_files = list(pdf_dir.glob("*.pdf"))
        pdf_count = len(pdf_files)
        if pdf_count > 0:
            st.info(f"üìÅ {pdf_count} file PDF s·∫µn s√†ng")
        else:
            st.warning("‚ö†Ô∏è Kh√¥ng c√≥ PDF n√†o trong data/pdf/")
    else:
        st.error("‚ùå Th∆∞ m·ª•c data/pdf/ kh√¥ng t·ªìn t·∫°i")
        st.info("T·∫°o th∆∞ m·ª•c: `mkdir data/pdf` v√† ƒë·∫∑t PDF v√†o ƒë√≥")
    
    # Run Embedding button
    if st.button("üöÄ Run Embedding", type="primary", help="Ch·∫°y embedding cho t·∫•t c·∫£ PDF"):
        try:
            # Initialize pipeline based on selected embedder
            embedder_type = st.session_state.get("embedder_type", "huggingface_local")
            
            with st.spinner(f"ƒêang ch·∫°y embedding v·ªõi {embedder_type}..."):
                from pipeline.rag_pipeline import RAGPipeline
                from embedders.embedder_type import EmbedderType
                
                # Map UI selection to pipeline parameters
                if embedder_type == "huggingface_local":
                    pipeline = RAGPipeline(
                        output_dir="data",
                        pdf_dir="data/pdf",
                        embedder_type=EmbedderType.HUGGINGFACE,
                        hf_use_api=False
                    )
                elif embedder_type == "huggingface_api":
                    pipeline = RAGPipeline(
                        output_dir="data",
                        pdf_dir="data/pdf",
                        embedder_type=EmbedderType.HUGGINGFACE,
                        hf_use_api=True
                    )
                else:  # ollama
                    pipeline = RAGPipeline(
                        output_dir="data",
                        pdf_dir="data/pdf",
                        embedder_type=EmbedderType.OLLAMA
                    )
                
                # Process all PDFs in directory
                pdf_dir = Path("data/pdf")
                results = pipeline.process_directory(pdf_dir)
                
                if results:
                    st.success(f"‚úÖ ƒê√£ x·ª≠ l√Ω {len(results)} file PDF!")
                    st.balloons()
                    
                    # Show results summary
                    with st.expander("üìä K·∫øt qu·∫£ x·ª≠ l√Ω"):
                        for result in results:
                            # Use correct keys from pipeline.process_pdf return dict
                            file_name = result.get('file_name', 'Unknown')
                            chunks = result.get('chunks', 0)
                            embeddings = result.get('embeddings', 0)
                            
                            st.write(f"üìÑ **{file_name}**")
                            st.write(f"   - Chunks: {chunks}")
                            st.write(f"   - Embeddings: {embeddings}")
                else:
                    st.warning("‚ö†Ô∏è Kh√¥ng c√≥ PDF n√†o ƒë∆∞·ª£c x·ª≠ l√Ω")
                    
        except Exception as e:
            st.error(f"‚ùå L·ªói embedding: {str(e)}")
            with st.expander("Chi ti·∫øt l·ªói"):
                import traceback
                st.code(traceback.format_exc())
    
    st.markdown("---")
    
    st.markdown("<div class='sidebar-footer'>", unsafe_allow_html=True)
    st.markdown("Welcome back", unsafe_allow_html=True)
    st.markdown("</div>", unsafe_allow_html=True)

# D√πng bi·∫øn backend th·ªëng nh·∫•t
backend = st.session_state["backend_mode"]

# === SESSION STATE INIT ===
if "messages" not in st.session_state:
    st.session_state["messages"] = []  # OpenAI format: [{"role": "user"/"assistant", "content": "..."}]
if "is_generating" not in st.session_state:
    st.session_state["is_generating"] = False
if "pending_prompt" not in st.session_state:
    st.session_state["pending_prompt"] = None
if "last_sources" not in st.session_state:
    st.session_state["last_sources"] = []
if "last_retrieval_info" not in st.session_state:
    st.session_state["last_retrieval_info"] = {}
if "last_queries" not in st.session_state:
    st.session_state["last_queries"] = []

# === CHAT HEADER ===
st.markdown("<div class='chat-header'>Chat Window</div>", unsafe_allow_html=True)

# === CHAT LOG RENDER ===
chat_html_parts = ["<div class='chat-log'>"]
for msg in st.session_state["messages"]:
    # Normalize role for display
    role = msg.get("role", "user")
    if role == "assistant":
        role = "bot"  # UI d√πng "bot" ƒë·ªÉ styling
    
    bubble = (
        f"<div class='chat-row {role}'><div class='chat-bubble {role}'>"
        f"{msg.get('content', '')}"
        "</div></div>"
    )
    chat_html_parts.append(bubble)

if st.session_state.get("is_generating") and st.session_state.get("pending_prompt"):
    chat_html_parts.append(
        "<div class='chat-row bot'><div class='chat-bubble bot'><span class='typing'>"
        "<span></span><span></span><span></span></span></div></div>"
    )

chat_html_parts.append("</div>")
st.markdown("".join(chat_html_parts), unsafe_allow_html=True)

# === RETRIEVAL SOURCES (UI) ===
sources = st.session_state.get("last_sources", [])
retrieval_info = st.session_state.get("last_retrieval_info", {})

if sources or retrieval_info:
    st.markdown("### Ngu·ªìn tham kh·∫£o")
    
    # Display retrieval info if available
    if retrieval_info:
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1:
            st.metric("Retrieved", retrieval_info.get("total_retrieved", 0))
        with col2:
            st.metric("Final Count", retrieval_info.get("final_count", 0))
        with col3:
            reranked_status = "‚úÖ Yes" if retrieval_info.get("reranked", False) else "‚ùå No"
            st.metric("Reranked", reranked_status)
        with col4:
            reranker = retrieval_info.get("reranker", "none")
            st.metric("Reranker", reranker[:10] + "..." if len(reranker) > 10 else reranker)
        with col5:
            qem_status = "‚úÖ Yes" if retrieval_info.get("query_enhanced", False) else "‚ùå No"
            st.metric("QEM", qem_status)

    # Display expanded queries if QEM was used
    if retrieval_info.get("query_enhanced", False):
        queries = st.session_state.get("last_queries", [])
        if len(queries) > 1:  # Only show if there are multiple queries (meaning expansion happened)
            with st.expander("üîç Expanded Queries (QEM)"):
                st.write("Query g·ªëc ƒë√£ ƒë∆∞·ª£c m·ªü r·ªông th√†nh:")
                for i, query in enumerate(queries, 1):
                    st.write(f"{i}. {query}")
    
    # Display sources
    for i, src in enumerate(sources, 1):
        file_name = src.get("file_name", "?")
        page = src.get("page_number", "?")
        
        # Get different score types
        hybrid_score = src.get("similarity_score", 0.0)
        vector_sim = src.get("vector_similarity")
        rerank_score = src.get("rerank_score")
        
        try:
            hybrid_score = float(hybrid_score)
        except (ValueError, TypeError):
            hybrid_score = 0.0
        
        # Build score display text
        score_parts = []
        
        # Show vector similarity (cosine) if available
        if vector_sim is not None:
            try:
                vector_sim = float(vector_sim)
                score_parts.append(f"Vec: {vector_sim:.4f}")
            except (ValueError, TypeError):
                pass
        
        # Show hybrid score (z-score weighted)
        score_parts.append(f"Hybrid: {hybrid_score:.4f}")
        
        # Show rerank score if available
        if rerank_score is not None:
            try:
                rerank_score = float(rerank_score)
                score_parts.append(f"Rerank: {rerank_score:.4f}")
            except (ValueError, TypeError):
                pass
        
        score_text = " | ".join(score_parts)
        
        text = src.get("snippet", "") or ""  # S·ª≠ d·ª•ng 'snippet' thay v√¨ 'text'
        snippet = text if len(text) <= 500 else text[:500] + "..."
        st.markdown(f"- [{i}] {file_name} - trang {page} ({score_text})")
        with st.expander(f"Xem tr√≠ch ƒëo·∫°n {i}"):
            if snippet.strip():
                st.markdown(snippet)
            else:
                st.write("Kh√¥ng c√≥ n·ªôi dung tr√≠ch ƒëo·∫°n")
else:
    st.info("Ch∆∞a c√≥ ngu·ªìn tham kh·∫£o n√†o ƒë∆∞·ª£c t√¨m th·∫•y. H√£y ƒë·∫∑t c√¢u h·ªèi ƒë·ªÉ h·ªá th·ªëng t√¨m ki·∫øm t√†i li·ªáu li√™n quan.")

# === BACKEND CALL ===
def ask_backend(prompt_text: str) -> str:
    """
    X·ª≠ l√Ω request t·ªõi LLM backend
    
    Args:
        prompt_text: User query
    
    Returns:
        Response t·ª´ LLM
    """
    try:
        # TODO: Khi c√≥ retrieval system, l·∫•y context ·ªü ƒë√¢y
        context = ""  # T·∫°m th·ªùi ƒë·ªÉ tr·ªëng
        
        # Build messages b·∫±ng chat_handler
        # L·∫•y context t·ª´ Retrieval (n·∫øu c√≥) v√† l∆∞u ngu·ªìn ƒë·ªÉ hi·ªÉn th·ªã.
        try:
            embedder_type = st.session_state.get("embedder_type", "huggingface_local")
            reranker_type = st.session_state.get("reranker_type", "bge_m3_hf_local")
            top_k_rerank = st.session_state.get("top_k_rerank", 5)
            use_query_enhancement = st.session_state.get("use_query_enhancement", True)
            
            # Collect API tokens for rerankers
            api_tokens = {}
            if reranker_type == "bge_m3_hf_api":
                try:
                    from embedders.providers.huggingface.token_manager import get_hf_token
                    token = get_hf_token()
                    api_tokens["hf"] = token
                    if token:
                        st.info(f"‚úÖ HF token loaded: {'***' + token[-4:]}")
                    else:
                        st.error("‚ùå HF token not found!")
                except Exception as e:
                    st.error(f"‚ùå Failed to get HF token: {e}")
            elif reranker_type == "cohere":
                token = os.getenv("COHERE_API_KEY") or os.getenv("COHERE_TOKEN")
                api_tokens["cohere"] = token
                if not token:
                    st.warning("‚ö†Ô∏è Cohere token not found in environment")
            elif reranker_type == "jina":
                token = os.getenv("JINA_API_KEY") or os.getenv("JINA_TOKEN")
                api_tokens["jina"] = token
                if not token:
                    st.warning("‚ö†Ô∏è Jina token not found in environment")
            
            ret = fetch_retrieval(
                prompt_text, 
                top_k=top_k_rerank,  # Use final top_k for simplified API
                max_chars=8000, 
                embedder_type=embedder_type, 
                reranker_type=reranker_type,
                use_query_enhancement=use_query_enhancement,
                api_tokens=api_tokens
            )
            context = ret.get("context", "") or ""
            st.session_state["last_sources"] = ret.get("sources", [])
            st.session_state["last_retrieval_info"] = ret.get("retrieval_info", {})
            st.session_state["last_queries"] = ret.get("queries", [])
        except Exception as e:
            st.error(f"L·ªói retrieval: {e}")
            context = ""
            st.session_state["last_sources"] = []
            st.session_state["last_retrieval_info"] = {}

        messages = build_messages(
            query=prompt_text,
            context=context,
            history=st.session_state["messages"]
        )
        
        # G·ªçi LLM t∆∞∆°ng ·ª©ng
        if backend == "gemini":
            reply = call_gemini(messages)
        else:  # lmstudio
            reply = call_lmstudio(messages)
        
        return reply
    
    except Exception as e:
        return f"[Error] {e}"

# === CHAT INPUT ===
prompt = st.chat_input("Type a new message here", disabled=st.session_state.get("is_generating", False))

if prompt and not st.session_state.get("is_generating", False):
    # Th√™m user message v√†o history (OpenAI format)
    st.session_state["messages"].append({"role": "user", "content": prompt})
    st.session_state["pending_prompt"] = prompt
    st.session_state["is_generating"] = True
    st.rerun()

# === GENERATE RESPONSE ===
if st.session_state.get("is_generating") and st.session_state.get("pending_prompt"):
    with st.spinner("Assistant is typing..."):
        reply = ask_backend(st.session_state["pending_prompt"])

    # Th√™m assistant response v√†o history (OpenAI format)
    st.session_state["messages"].append({"role": "assistant", "content": reply})
    st.session_state["pending_prompt"] = None
    st.session_state["is_generating"] = False
    st.rerun()