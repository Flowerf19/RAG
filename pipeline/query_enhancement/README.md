# Query Enhancement Module (QEM)# Query Enhancement Module (QEM)



[![Python Version](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)[![Python Version](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)

[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)



Module m·ªü r·ªông truy v·∫•n (Query Enhancement) cho h·ªá th·ªëng RAG, s·ª≠ d·ª•ng LLM ƒë·ªÉ t·∫°o nhi·ªÅu bi·∫øn th·ªÉ truy v·∫•n ƒëa ng√¥n ng·ªØ tr∆∞·ªõc khi t√¨m ki·∫øm. T√≠ch h·ª£p v·ªõi FAISS vector search v√† BM25 keyword search ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c retrieval.Module m·ªü r·ªông truy v·∫•n (Query Enhancement) cho h·ªá th·ªëng RAG, s·ª≠ d·ª•ng LLM ƒë·ªÉ t·∫°o nhi·ªÅu bi·∫øn th·ªÉ truy v·∫•n ƒëa ng√¥n ng·ªØ tr∆∞·ªõc khi t√¨m ki·∫øm. T√≠ch h·ª£p v·ªõi FAISS vector search v√† BM25 keyword search ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c retrieval.



## ‚ú® T√≠nh nƒÉng ch√≠nh## ‚ú® T√≠nh nƒÉng ch√≠nh



- üîç **Multi-Language Query Expansion**: T·∫°o bi·∫øn th·ªÉ truy v·∫•n b·∫±ng ti·∫øng Vi·ªát v√† ti·∫øng Anh- üîç **Multi-Language Query Expansion**: T·∫°o bi·∫øn th·ªÉ truy v·∫•n b·∫±ng ti·∫øng Vi·ªát v√† ti·∫øng Anh

- ü§ñ **Multi-LLM Backend**: H·ªó tr·ª£ Gemini, LM Studio, v√† c√°c LLM kh√°c- ü§ñ **Multi-LLM Backend**: H·ªó tr·ª£ Gemini, LM Studio, v√† c√°c LLM kh√°c

- üìä **Activity Logging**: Ghi log JSONL chi ti·∫øt cho monitoring v√† debugging- üìä **Activity Logging**: Ghi log JSONL chi ti·∫øt cho monitoring v√† debugging

- ‚öôÔ∏è **Flexible Configuration**: YAML config v·ªõi runtime override- ‚öôÔ∏è **Flexible Configuration**: YAML config v·ªõi runtime override

- üîÑ **Graceful Fallback**: Fallback v·ªÅ truy v·∫•n g·ªëc khi LLM l·ªói- üîÑ **Graceful Fallback**: Fallback v·ªÅ truy v·∫•n g·ªëc khi LLM l·ªói

- üéØ **Intent Preservation**: Gi·ªØ nguy√™n √Ω ƒë·ªãnh truy v·∫•n g·ªëc khi m·ªü r·ªông- üéØ **Intent Preservation**: Gi·ªØ nguy√™n √Ω ƒë·ªãnh truy v·∫•n g·ªëc khi m·ªü r·ªông



## üöÄ Kh·ªüi ƒë·ªông nhanh## üöÄ Kh·ªüi ƒë·ªông nhanh



### Y√™u c·∫ßu### Y√™u c·∫ßu



- **Python**: >= 3.13- **Python**: >= 3.13

- **LLM Backend**: Gemini API key ho·∫∑c LM Studio server- **LLM Backend**: Gemini API key ho·∫∑c LM Studio server

- **Dependencies**: PyYAML, requests- **Dependencies**: PyYAML, requests



### C·∫•u h√¨nh c∆° b·∫£n### C·∫•u h√¨nh c∆° b·∫£n



```yaml```yaml

# qem_config.yaml# qem_config.yaml

enabled: trueenabled: true

languages:languages:

  vi: 2    # 2 bi·∫øn th·ªÉ ti·∫øng Vi·ªát  vi: 2    # 2 bi·∫øn th·ªÉ ti·∫øng Vi·ªát

  en: 2    # 2 bi·∫øn th·ªÉ ti·∫øng Anh  en: 2    # 2 bi·∫øn th·ªÉ ti·∫øng Anh

max_total_queries: 5max_total_queries: 5

backend: geminibackend: gemini

fallback_backend: geminifallback_backend: gemini

``````



### S·ª≠ d·ª•ng c∆° b·∫£n### S·ª≠ d·ª•ng c∆° b·∫£n



```python```python

from pipeline.query_enhancement import QueryEnhancementModule, load_qem_settingsfrom pipeline.query_enhancement import QueryEnhancementModule, load_qem_settings



# Load config# Load config

qem_settings = load_qem_settings()qem_settings = load_qem_settings()



# Initialize module# Initialize module

qem = QueryEnhancementModule(app_config={}, qem_settings=qem_settings)qem = QueryEnhancementModule(app_config={}, qem_settings=qem_settings)



# Enhance query# Enhance query

original_query = "qu·∫£n l√Ω r·ªßi ro trong IT"original_query = "qu·∫£n l√Ω r·ªßi ro trong IT"

enhanced_queries = qem.enhance(original_query)enhanced_queries = qem.enhance(original_query)

# Returns: ["qu·∫£n l√Ω r·ªßi ro trong IT", "IT risk management", "qu·∫£n l√Ω r·ªßi ro CNTT", ...]# Returns: ["qu·∫£n l√Ω r·ªßi ro trong IT", "IT risk management", "qu·∫£n l√Ω r·ªßi ro CNTT", ...]

``````



## üìÅ C·∫•u tr√∫c module## üìÅ C·∫•u tr√∫c module



``````

query_enhancement/query_enhancement/

‚îú‚îÄ‚îÄ __init__.py              # Export ch√≠nh: QueryEnhancementModule, load_qem_settings‚îú‚îÄ‚îÄ __init__.py              # Export ch√≠nh: QueryEnhancementModule, load_qem_settings

‚îú‚îÄ‚îÄ qem_core.py              # L·ªõp ƒëi·ªÅu ph·ªëi trung t√¢m‚îú‚îÄ‚îÄ qem_core.py              # L·ªõp ƒëi·ªÅu ph·ªëi trung t√¢m

‚îú‚îÄ‚îÄ qem_lm_client.py         # LLM client wrapper (Gemini, LM Studio)‚îú‚îÄ‚îÄ qem_lm_client.py         # LLM client wrapper (Gemini, LM Studio)

‚îú‚îÄ‚îÄ qem_strategy.py          # Prompt building strategy‚îú‚îÄ‚îÄ qem_strategy.py          # Prompt building strategy

‚îú‚îÄ‚îÄ qem_utils.py             # Utility functions (parse, dedup, logging)‚îú‚îÄ‚îÄ qem_utils.py             # Utility functions (parse, dedup, logging)

‚îú‚îÄ‚îÄ qem_config.yaml          # Default configuration‚îú‚îÄ‚îÄ qem_config.yaml          # Default configuration

‚îî‚îÄ‚îÄ README.md               # Documentation‚îî‚îÄ‚îÄ README.md               # Documentation

``````



### Data Flow Architecture### Data Flow Architecture



```mermaid```mermaid

graph TDgraph TD

    A[User Query] --> B[QueryEnhancementModule]    A[User Query] --> B[QueryEnhancementModule]

    B --> C[Build Prompt]    B --> C[Build Prompt]

    C --> D[QEMLLMClient]    C --> D[QEMLLMClient]

    D --> E{Backend}    D --> E{Backend}

    E -->|Gemini| F[Gemini API]    E -->|Gemini| F[Gemini API]

    E -->|LM Studio| G[LM Studio Local]    E -->|LM Studio| G[LM Studio Local]

    F --> H[Raw Response]    F --> H[Raw Response]

    G --> H    G --> H

    H --> I[Parse & Process]    H --> I[Parse & Process]

    I --> J[Deduplicate]    I --> J[Deduplicate]

    J --> K[Add Original]    J --> K[Add Original]

    K --> L[Clip to Max]    K --> L[Clip to Max]

    L --> M[Enhanced Queries]    L --> M[Enhanced Queries]



    M --> N[FAISS Embedding Fusion]    M --> N[FAISS Embedding Fusion]

    M --> O[BM25 Query Concat]    M --> O[BM25 Query Concat]



    style A fill:#e1f5fe    style A fill:#e1f5fe

    style M fill:#c8e6c9    style M fill:#c8e6c9

``````



## üîß S·ª≠ d·ª•ng trong code## ‚öôÔ∏è C·∫•u h√¨nh



### Basic Usage### qem_config.yaml



```python```yaml

from pipeline.query_enhancement import QueryEnhancementModule, load_qem_settings# Enable/disable QEM

enabled: true

# Load settings from YAML

qem_settings = load_qem_settings()# Language variants to generate

languages:

# Initialize with app config  vi: 2        # Vietnamese variants

app_config = {}  # Your app configuration  en: 2        # English variants

qem = QueryEnhancementModule(app_config, qem_settings)

# Maximum total queries (including original)

# Enhance single querymax_total_queries: 5

query = "service management process"

enhanced = qem.enhance(query)# LLM Backend selection

print(f"Original: {query}")backend: gemini                    # Primary backend

print(f"Enhanced: {enhanced}")fallback_backend: gemini          # Fallback if primary fails

```

# LLM parameters override

### Integration v·ªõi RAG Pipelinellm_overrides:

  model_name: "gemini-1.5-flash"

```python  temperature: 0.3

# Trong backend_connector.py  max_tokens: 200

from pipeline.query_enhancement import QueryEnhancementModule, load_qem_settings

# Additional prompt instructions

def fetch_retrieval(query, ...):additional_instructions: |

    # Load QEM settings  Focus on IT service management terminology.

    qem_settings = load_qem_settings()  Include synonyms for technical terms.



    # Initialize QEM# Logging configuration

    qem = QueryEnhancementModule(app_config, qem_settings)log_path: "data/logs/qem_activity.jsonl"

```

    # Enhance query

    enhanced_queries = qem.enhance(query)### Environment Variables



    # Use enhanced queries for retrieval```bash

    fused_embedding = _fuse_query_embeddings(enhanced_queries)# Gemini API (required for gemini backend)

    bm25_query = " ".join(enhanced_queries)export GOOGLE_API_KEY="your-gemini-api-key"



    # Continue with FAISS + BM25 search...# LM Studio (required for lmstudio backend)

```export LM_STUDIO_BASE_URL="http://localhost:1234"

```

### Custom Configuration

### 3.2 `QEMLLMClient` (`qem_lm_client.py`)

```python- **Nhi·ªám v·ª•**: t·∫ßng adapter quy·∫øt ƒë·ªãnh ch·ªçn backend v√† truy·ªÅn th√¥ng s·ªë ƒë·∫øn h√†m g·ªçi LLM chung c·ªßa h·ªá th·ªëng.

# Override settings programmatically- **L·ª±a ch·ªçn backend** (`_resolve_backend`):

custom_settings = {  1. ∆Øu ti√™n `qem_config["backend"]` n·∫øu ƒë∆∞·ª£c ƒë·∫∑t.

    "enabled": True,  2. N·∫øu kh√¥ng, ƒë·ªçc `app_config["ui"]["default_backend"]`.

    "languages": {"vi": 3, "en": 1},  3. Fallback cu·ªëi c√πng: `qem_config["fallback_backend"]` (m·∫∑c ƒë·ªãnh `gemini`).

    "max_total_queries": 5,- **L·ªùi g·ªçi Gemini** (`_call_gemini`):

    "backend": "lmstudio",  - G·ª≠i messages v·ªõi `system_prompt` v√† prompt ng∆∞·ªùi d√πng.

    "llm_overrides": {  - Cho ph√©p override `model_name`, `temperature`, `max_tokens`.

        "temperature": 0.3,- **L·ªùi g·ªçi LM Studio** (`_call_lmstudio`):

        "max_tokens": 200  - Convert c√°c tham s·ªë s·ªë h·ªçc sang ki·ªÉu ph√π h·ª£p.

    }  - Tr·∫£ v·ªÅ chu·ªói vƒÉn b·∫£n raw ƒë·ªÉ caller t·ª± parse.

}

### 3.3 Prompt strategy (`qem_strategy.py`)

qem = QueryEnhancementModule(app_config, custom_settings)- H√†m `build_prompt` nh·∫≠n truy v·∫•n v√† b·∫£n ƒë·ªì `{ng√¥n_ng·ªØ: s·ªë_l∆∞·ª£ng}`.

```- T√≠nh t·ªïng s·ªë bi·∫øn th·ªÉ c·∫ßn sinh, chu·∫©n h√≥a m√¥ t·∫£ ng√¥n ng·ªØ (English/Vietnamese).  

- Gh√©p h∆∞·ªõng d·∫´n b·∫Øt bu·ªôc:

## ‚öôÔ∏è C·∫•u h√¨nh  - Kh√¥ng thay ƒë·ªïi √Ω ƒë·ªãnh.

  - Gi·ªØ c√¢u ng·∫Øn (‚â§ 25 t·ª´).

### qem_config.yaml  - Output **ph·∫£i** l√† JSON array.

- Cho ph√©p ch√®n th√™m h∆∞·ªõng d·∫´n t·ª± do (`additional_instructions`) t·ª´ c·∫•u h√¨nh.

```yaml

# Enable/disable QEM### 3.4 Ti·ªán √≠ch (`qem_utils.py`)

enabled: true- `normalize_query` / `deduplicate_queries`: Chu·∫©n h√≥a v√† lo·∫°i b·ªè tr√πng l·∫∑p nh∆∞ng v·∫´n gi·ªØ nguy√™n casing ƒë·∫ßu ra cho hi·ªÉn th·ªã.

- `parse_llm_list`: H·ªó tr·ª£ c·∫£ JSON array l·∫´n danh s√°ch d·∫°ng bullet/ƒë√°nh s·ªë.

# Language variants to generate- `clip_queries`: C·∫Øt danh s√°ch theo `max_total_queries`.

languages:- `log_activity`: ƒê·∫£m b·∫£o th∆∞ m·ª•c log t·ªìn t·∫°i, append payload d·∫°ng JSON line v√†o `log_path`. H·ªó tr·ª£ ti·∫øng Vi·ªát ho·∫∑c Unicode nh·ªù `ensure_ascii=False`.

  vi: 2        # Vietnamese variants- `summarise_queries`: T·∫°o chu·ªói g·ªçn g√†ng ph·ª•c v·ª• logging (`logger.debug/info` t·ª´ core).

  en: 2        # English variants

### 3.5 C·∫•u h√¨nh (`qem_config.yaml`)

# Maximum total queries (including original)- `enabled`: B·∫≠t/t·∫Øt QEM ·ªü runtime.

max_total_queries: 5- `languages`: S·ªë bi·∫øn th·ªÉ mong mu·ªën cho t·ª´ng m√£ ng√¥n ng·ªØ (v√≠ d·ª• `vi: 2`, `en: 2`).

- `max_total_queries`: Gi·ªõi h·∫°n c·ª©ng s·ªë truy v·∫•n tr·∫£ v·ªÅ (bao g·ªìm truy v·∫•n g·ªëc).

# LLM Backend selection- `backend` & `fallback_backend`: ƒêi·ªÅu khi·ªÉn backend LLM ƒë∆∞·ª£c ch·ªçn.

backend: gemini                    # Primary backend- `llm_overrides`: T√πy bi·∫øn th√¥ng s·ªë g·ªçi LLM (temperature, max_tokens, model‚Ä¶).

fallback_backend: gemini          # Fallback if primary fails- `additional_instructions`: Chu·ªói h∆∞·ªõng d·∫´n th√™m, append v√†o prompt.

- `log_path`: ƒê∆∞·ªùng d·∫´n log JSONL (`data/logs/qem_activity.jsonl` theo c·∫•u h√¨nh m·∫´u).

# LLM parameters override

llm_overrides:## üö® Troubleshooting

  model_name: "gemini-1.5-flash"

  temperature: 0.3### Common Issues

  max_tokens: 200

#### LLM Backend Connection Failed

# Additional prompt instructions

additional_instructions: |```python

  Focus on IT service management terminology.# Check backend availability

  Include synonyms for technical terms.from pipeline.query_enhancement.qem_lm_client import QEMLLMClient



# Logging configurationclient = QEMLLMClient(app_config, qem_settings)

log_path: "data/logs/qem_activity.jsonl"try:

```    test_response = client.generate_variants("test query", {"en": 1})

    print("Backend working:", test_response)

### Environment Variablesexcept Exception as e:

    print("Backend error:", e)

```bash```

# Gemini API (required for gemini backend)

export GOOGLE_API_KEY="your-gemini-api-key"#### Invalid YAML Configuration



# LM Studio (required for lmstudio backend)```python

export LM_STUDIO_BASE_URL="http://localhost:1234"# Validate config

```import yaml

from pipeline.query_enhancement import load_qem_settings

## üìä Monitoring & Logging

try:

### Activity Logs    settings = load_qem_settings()

    print("Config loaded successfully")

QEM ghi log chi ti·∫øt v√†o `data/logs/qem_activity.jsonl`:    print("Languages:", settings.get("languages"))

except yaml.YAMLError as e:

```json    print("YAML error:", e)

{```

  "timestamp": "2025-10-28T10:30:00Z",

  "backend": "gemini",#### No Query Variants Generated

  "original_query": "IT service management",

  "enhanced_queries": ["IT service management", "ITSM", "qu·∫£n l√Ω d·ªãch v·ª• CNTT"],```python

  "raw_response": "[\"IT service management\", \"ITSM\", \"qu·∫£n l√Ω d·ªãch v·ª• CNTT\"]",# Debug QEM processing

  "error": null,qem = QueryEnhancementModule(app_config, qem_settings)

  "processing_time_ms": 1250

}# Enable debug logging

```import logging

logging.basicConfig(level=logging.DEBUG)

### Log Analysis

result = qem.enhance("test query")

```pythonprint("Result:", result)

import json```



# Read QEM activity logs### Performance Tuning

with open("data/logs/qem_activity.jsonl", "r", encoding="utf-8") as f:

    for line in f:```yaml

        entry = json.loads(line)# High performance config

        print(f"Query: {entry['original_query']}")enabled: true

        print(f"Enhanced: {len(entry['enhanced_queries'])} variants")languages:

        print(f"Backend: {entry['backend']}")  vi: 1

        print("---")  en: 1

```max_total_queries: 3

llm_overrides:

## üö® Troubleshooting  temperature: 0.1      # Lower temperature for consistency

  max_tokens: 100       # Shorter responses

### Common Issues```



#### LLM Backend Connection Failed## üß™ Testing



```python### Unit Tests

# Check backend availability

from pipeline.query_enhancement.qem_lm_client import QEMLLMClient```python

# Test QEM components

client = QEMLLMClient(app_config, qem_settings)from pipeline.query_enhancement.qem_utils import parse_llm_list, deduplicate_queries

try:

    test_response = client.generate_variants("test query", {"en": 1})# Test parsing

    print("Backend working:", test_response)raw_response = '["query1", "query2", "query1"]'

except Exception as e:parsed = parse_llm_list(raw_response)

    print("Backend error:", e)assert parsed == ["query1", "query2", "query1"]

```

# Test deduplication

#### Invalid YAML Configurationdeduped = deduplicate_queries(parsed)

assert deduped == ["query1", "query2"]

```python```

# Validate config

import yaml### Integration Tests

from pipeline.query_enhancement import load_qem_settings

```python

try:# Full QEM pipeline test

    settings = load_qem_settings()from pipeline.query_enhancement import QueryEnhancementModule

    print("Config loaded successfully")

    print("Languages:", settings.get("languages"))qem = QueryEnhancementModule({}, {"enabled": True, "languages": {"en": 1}})

except yaml.YAMLError as e:result = qem.enhance("test query")

    print("YAML error:", e)

```assert len(result) >= 1  # At least original query

assert "test query" in result  # Original preserved

#### No Query Variants Generated```



```python## üîß Development

# Debug QEM processing

qem = QueryEnhancementModule(app_config, qem_settings)### Adding New LLM Backend



# Enable debug logging```python

import logging# In qem_lm_client.py

logging.basicConfig(level=logging.DEBUG)def _call_new_backend(self, prompt, **kwargs):

    # Implement new backend logic

result = qem.enhance("test query")    response = call_new_llm_api(prompt, **kwargs)

print("Result:", result)    return response

```

# Update _resolve_backend method

### Performance Tuningdef _resolve_backend(self):

    # Add new backend option

```yaml    if self.config.get("backend") == "new_backend":

# High performance config        return self._call_new_backend

enabled: true```

languages:

  vi: 1### Custom Prompt Strategy

  en: 1

max_total_queries: 3```python

llm_overrides:# In qem_strategy.py

  temperature: 0.1      # Lower temperature for consistencydef build_custom_prompt(query, language_map, additional_instructions=""):

  max_tokens: 100       # Shorter responses    # Custom prompt building logic

```    prompt = f"Generate variants for: {query}\n"

    prompt += f"Languages: {language_map}\n"

## üß™ Testing    if additional_instructions:

        prompt += f"Additional: {additional_instructions}\n"

### Unit Tests    return prompt

```

```python

# Test QEM components## üìà Performance Metrics

from pipeline.query_enhancement.qem_utils import parse_llm_list, deduplicate_queries

- **Response Time**: 500-2000ms per query (depends on LLM backend)

# Test parsing- **Success Rate**: >95% v·ªõi Gemini, >90% v·ªõi LM Studio

raw_response = '["query1", "query2", "query1"]'- **Query Expansion**: 2-5x s·ªë l∆∞·ª£ng truy v·∫•n

parsed = parse_llm_list(raw_response)- **Memory Usage**: <50MB cho module

assert parsed == ["query1", "query2", "query1"]

## ü§ù Contributing

# Test deduplication

deduped = deduplicate_queries(parsed)### Code Standards

assert deduped == ["query1", "query2"]

```- **Language**: Vietnamese comments, English docstrings

- **Style**: Black formatter, isort imports

### Integration Tests- **Testing**: pytest v·ªõi coverage > 80%

- **Documentation**: Update README cho breaking changes

```python

# Full QEM pipeline test### Architecture Guidelines

from pipeline.query_enhancement import QueryEnhancementModule

- **Single Responsibility**: M·ªói module m·ªôt nhi·ªám v·ª• r√µ r√†ng

qem = QueryEnhancementModule({}, {"enabled": True, "languages": {"en": 1}})- **Error Handling**: Graceful degradation, detailed logging

result = qem.enhance("test query")- **Configuration**: YAML-first v·ªõi programmatic override

- **Testing**: Unit tests cho utilities, integration tests cho pipeline

assert len(result) >= 1  # At least original query

assert "test query" in result  # Original preserved## üìû Support

```

- **Issues**: [GitHub Issues](https://github.com/Flowerf19/RAG/issues)

## üîß Development- **Discussions**: [GitHub Discussions](https://github.com/Flowerf19/RAG/discussions)

- **Documentation**: See module READMEs for technical details

### Adding New LLM Backend

---

```python

# In qem_lm_client.py*Module n√†y l√† ph·∫ßn c·ªßa h·ªá th·ªëng RAG Pipeline. Xem README ch√≠nh ƒë·ªÉ bi·∫øt th√™m v·ªÅ ki·∫øn tr√∫c t·ªïng th·ªÉ.*

def _call_new_backend(self, prompt, **kwargs):
    # Implement new backend logic
    response = call_new_llm_api(prompt, **kwargs)
    return response

# Update _resolve_backend method
def _resolve_backend(self):
    # Add new backend option
    if self.config.get("backend") == "new_backend":
        return self._call_new_backend
```

### Custom Prompt Strategy

```python
# In qem_strategy.py
def build_custom_prompt(query, language_map, additional_instructions=""):
    # Custom prompt building logic
    prompt = f"Generate variants for: {query}\n"
    prompt += f"Languages: {language_map}\n"
    if additional_instructions:
        prompt += f"Additional: {additional_instructions}\n"
    return prompt
```

## üìà Performance Metrics

- **Response Time**: 500-2000ms per query (depends on LLM backend)
- **Success Rate**: >95% v·ªõi Gemini, >90% v·ªõi LM Studio
- **Query Expansion**: 2-5x s·ªë l∆∞·ª£ng truy v·∫•n
- **Memory Usage**: <50MB cho module

## ü§ù Contributing

### Code Standards

- **Language**: Vietnamese comments, English docstrings
- **Style**: Black formatter, isort imports
- **Testing**: pytest v·ªõi coverage > 80%
- **Documentation**: Update README cho breaking changes

### Architecture Guidelines

- **Single Responsibility**: M·ªói module m·ªôt nhi·ªám v·ª• r√µ r√†ng
- **Error Handling**: Graceful degradation, detailed logging
- **Configuration**: YAML-first v·ªõi programmatic override
- **Testing**: Unit tests cho utilities, integration tests cho pipeline

## üìû Support

- **Issues**: [GitHub Issues](https://github.com/Flowerf19/RAG/issues)
- **Discussions**: [GitHub Discussions](https://github.com/Flowerf19/RAG/discussions)
- **Documentation**: See module READMEs for technical details

---

*Module n√†y l√† ph·∫ßn c·ªßa h·ªá th·ªëng RAG Pipeline. Xem README ch√≠nh ƒë·ªÉ bi·∫øt th√™m v·ªÅ ki·∫øn tr√∫c t·ªïng th·ªÉ.*