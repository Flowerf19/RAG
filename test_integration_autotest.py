#!/usr/bin/env python3
"""
Integration test for autotest functionality - simulates full workflow
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
from io import StringIO
from evaluation.backend_dashboard.api import BackendDashboard
from ui.dashboard.components.ground_truth.file_handler import normalize_columns

def simulate_file_upload_and_autotest():
    """Simulate the complete autotest workflow"""
    print("ğŸš€ Simulating complete autotest workflow...")
    print("=" * 60)

    # Step 1: Create test CSV data
    print("ğŸ“„ Step 1: Creating test CSV data...")
    csv_data = """STT,CÃ¢u há»i,CÃ¢u tráº£ lá»i,Nguá»“n
1,Machine learning lÃ  gÃ¬?,Machine learning lÃ  má»™t nhÃ¡nh cá»§a trÃ­ tuá»‡ nhÃ¢n táº¡o cho phÃ©p mÃ¡y tÃ­nh há»c tá»« dá»¯ liá»‡u.,AI Basics
2,Deep learning khÃ¡c gÃ¬ vá»›i machine learning?,Deep learning lÃ  má»™t subset cá»§a machine learning sá»­ dá»¥ng neural networks vá»›i nhiá»u layers.,Neural Networks
3,CNN Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ lÃ m gÃ¬?,CNN (Convolutional Neural Networks) Ä‘Æ°á»£c sá»­ dá»¥ng chá»§ yáº¿u cho computer vision tasks nhÆ° image recognition.,Computer Vision"""

    df = pd.read_csv(StringIO(csv_data))
    print(f"âœ… Created test data with {len(df)} rows")

    # Step 2: Normalize columns (simulate file parsing)
    print("ğŸ”„ Step 2: Normalizing columns...")
    normalized = normalize_columns(df)
    print(f"âœ… Normalized data: {list(normalized.columns)}")
    print(f"   Sample question: {normalized.iloc[0]['question'][:50]}...")

    # Step 3: Initialize backend and handlers
    print("ğŸ”§ Step 3: Initializing backend and handlers...")
    backend = BackendDashboard()
    print("âœ… Components initialized")

    # Step 4: Simulate auto-import (what happens when auto_import=True)
    print("ğŸ’¾ Step 4: Simulating auto-import to database...")
    rows = []
    for _, r in normalized.iterrows():
        rows.append({
            'question': r.get('question', ''),
            'answer': r.get('answer', ''),
            'source': r.get('source', '')
        })

    try:
        inserted = backend.insert_ground_truth_rows(rows)
        print(f"âœ… Auto-import successful: {inserted} rows inserted")
    except Exception as e:
        print(f"âŒ Auto-import failed: {e}")
        return False

    # Step 5: Verify data in database
    print("ğŸ” Step 5: Verifying data in database...")
    ground_truth_list = backend.get_ground_truth_list(limit=10)
    print(f"âœ… Found {len(ground_truth_list)} entries in database")

    # Step 6: Simulate auto-run evaluation (what happens when auto_run_eval=True)
    print("âš¡ Step 6: Simulating auto-run evaluation...")
    try:
        # Use minimal settings for quick test
        eval_result = backend.evaluate_ground_truth_with_ragas(
            llm_provider='ollama',
            model_name='gemma3:1b',
            limit=2,  # Only test 2 samples for speed
            save_to_db=True
        )

        if 'error' in eval_result:
            print(f"âŒ Evaluation failed: {eval_result['error']}")
            return False

        print("âœ… Auto-evaluation successful!")
        print(f"   Total samples: {eval_result.get('total_samples', 0)}")
        print(f"   Faithfulness: {eval_result.get('faithfulness', {}).get('mean', 'N/A'):.3f}")
        print(f"   Context Recall: {eval_result.get('context_recall', {}).get('mean', 'N/A'):.3f}")
        print(f"   Context Relevance: {eval_result.get('context_relevance', {}).get('mean', 'N/A'):.3f}")
        print(f"   Answer Relevancy: {eval_result.get('answer_relevancy', {}).get('mean', 'N/A'):.3f}")

    except Exception as e:
        print(f"âŒ Evaluation failed: {e}")
        return False

    # Step 7: Final verification
    print("ğŸ¯ Step 7: Final verification...")
    print("âœ… File upload simulation: PASSED")
    print("âœ… Column normalization: PASSED")
    print("âœ… Auto-import to DB: PASSED")
    print("âœ… Auto-run evaluation: PASSED")
    print("âœ… Results saved to DB: PASSED")

    print("=" * 60)
    print("ğŸ‰ COMPLETE AUTOTEST WORKFLOW SUCCESSFUL!")
    print()
    print("ğŸ“‹ Summary:")
    print(f"   - Processed {len(normalized)} ground truth Q&A pairs")
    print("   - Auto-imported to database")
    print("   - Auto-ran Ragas evaluation with 4 metrics")
    print("   - Saved results for dashboard analytics")

    return True

def test_edge_cases():
    """Test edge cases"""
    print("ğŸ§ª Testing edge cases...")

    # Test empty CSV
    try:
        empty_df = pd.DataFrame(columns=['STT', 'CÃ¢u há»i', 'CÃ¢u tráº£ lá»i', 'Nguá»“n'])
        normalized = normalize_columns(empty_df)
        assert len(normalized) == 0, "Empty dataframe should remain empty"
        print("âœ… Empty CSV handling: PASSED")
    except Exception as e:
        print(f"âŒ Empty CSV test failed: {e}")

    # Test missing columns
    try:
        incomplete_df = pd.DataFrame({'STT': [1], 'Question': ['Q1']})  # Missing answer and source
        normalized = normalize_columns(incomplete_df)
        assert 'question' in normalized.columns, "Should create question column"
        assert normalized.iloc[0]['answer'] == '', "Missing columns should be empty strings"
        print("âœ… Missing columns handling: PASSED")
    except Exception as e:
        print(f"âŒ Missing columns test failed: {e}")

def main():
    """Run integration tests"""
    print("ğŸ§ª UI AUTOTEST INTEGRATION TESTS")
    print("=" * 60)

    try:
        # Run main workflow test
        success = simulate_file_upload_and_autotest()

        if success:
            # Run edge case tests
            test_edge_cases()

            print("=" * 60)
            print("ğŸ¯ ALL INTEGRATION TESTS PASSED!")
            print()
            print("âœ… UI Autotest functionality is fully operational:")
            print("   - File upload â†’ Auto-parse â†’ Auto-import â†’ Auto-evaluate â†’ Save results")
            print("   - Handles Vietnamese column names correctly")
            print("   - Works with Ollama Gemma3:1b for evaluation")
            print("   - Saves all 4 Ragas metrics to database")
        else:
            print("âŒ Integration test failed!")
            return 1

    except Exception as e:
        print(f"âŒ Test suite failed: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0

if __name__ == "__main__":
    exit(main())