#!/usr/bin/env python3
"""
Test há»‡ thá»‘ng RAG vá»›i cÃ¢u há»i 11-15 (pháº§n cÃ²n láº¡i)
"""

import sys
import os
import json
from pathlib import Path

# Add current directory to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

try:
    from pipeline.backend_connector import fetch_retrieval
    from llm.chat_handler import build_messages
    from llm.LLM_API import call_gemini
    print("âœ… All imports successful")
except ImportError as e:
    print(f"âŒ Import failed: {e}")
    sys.exit(1)

# CÃ¢u há»i 11-15 (pháº§n cÃ²n láº¡i)
TEST_QUESTIONS = [
    {
        "id": 11,
        "question": "Cache-Augmented Generation (CAG) hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o Ä‘á»ƒ tá»‘i Æ°u hÃ³a chi phÃ­ vÃ  Ä‘á»™ trá»… cho há»‡ thá»‘ng RAG, vÃ  ká»¹ thuáº­t nÃ y sá»­ dá»¥ng nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ o Ä‘á»ƒ xÃ¡c Ä‘á»‹nh vÃ  lÆ°u trá»¯ cÃ¡c cÃ¢u tráº£ lá»i cÃ³ thá»ƒ tÃ¡i sá»­ dá»¥ng?",
        "topic": "Ká»¹ thuáº­t Cache-Augmented Generation (CAG)"
    },
    {
        "id": 12,
        "question": "Ká»¹ thuáº­t Speculative Retrieval (Truy xuáº¥t Dá»± Ä‘oÃ¡n) hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t cá»§a há»‡ thá»‘ng RAG, vÃ  nÃ³ sá»­ dá»¥ng nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ o Ä‘á»ƒ dá»± Ä‘oÃ¡n vÃ  má»Ÿ rá»™ng táº­p tÃ i liá»‡u liÃªn quan?",
        "topic": "Ká»¹ thuáº­t Speculative Retrieval"
    },
    {
        "id": 13,
        "question": "Ká»¹ thuáº­t Step-Back Prompting giáº£i quyáº¿t váº¥n Ä‘á» gÃ¬ cá»§a RAG cÆ¡ báº£n, vÃ  cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a nÃ³ khÃ¡c vá»›i cÃ¡c ká»¹ thuáº­t prompting khÃ¡c nhÆ° Chain-of-Thought nhÆ° tháº¿ nÃ o?",
        "topic": "Ká»¹ thuáº­t Step-Back Prompting"
    },
    {
        "id": 14,
        "question": "Ká»¹ thuáº­t Sub-Question Query Engine (SQE) hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o Ä‘á»ƒ cáº£i thiá»‡n kháº£ nÄƒng tráº£ lá»i cÃ¢u há»i phá»©c táº¡p trong há»‡ thá»‘ng RAG, vÃ  nÃ³ sá»­ dá»¥ng nhá»¯ng chiáº¿n lÆ°á»£c nÃ o Ä‘á»ƒ phÃ¢n tÃ­ch vÃ  tá»•ng há»£p thÃ´ng tin?",
        "topic": "Ká»¹ thuáº­t Sub-Question Query Engine (SQE)"
    },
    {
        "id": 15,
        "question": "Ká»¹ thuáº­t Iterative Retrieval-Generation (IRG) khÃ¡c vá»›i kiáº¿n trÃºc RAG cÆ¡ báº£n nhÆ° tháº¿ nÃ o, vÃ  nÃ³ sá»­ dá»¥ng nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ o Ä‘á»ƒ tinh chá»‰nh dáº§n dáº§n quÃ¡ trÃ¬nh truy xuáº¥t vÃ  táº¡o ra cÃ¢u tráº£ lá»i?",
        "topic": "Ká»¹ thuáº­t Iterative Retrieval-Generation (IRG)"
    }
]

def test_rag_system_part2():
    """Test há»‡ thá»‘ng RAG vá»›i cÃ¢u há»i 11-15"""
    results = []

    print("ðŸš€ Testing RAG System with Questions 11-15")
    print("=" * 60)

    for i, test_case in enumerate(TEST_QUESTIONS, 1):
        question_id = test_case["id"]
        question = test_case["question"]
        topic = test_case["topic"]

        print(f"\nðŸ“‹ Question {question_id}: {topic}")
        print(f"â“ {question[:100]}{'...' if len(question) > 100 else ''}")

        try:
            # Step 1: Get retrieval data
            print("ðŸ” Retrieving context...")
            ret = fetch_retrieval(question, top_k=10, max_chars=8000)
            context = ret.get("context", "") or ""
            sources = ret.get("sources", [])

            print(f"   ðŸ“„ Context: {len(context)} chars")
            print(f"   ðŸ“š Sources: {len(sources)} items")

            # Step 2: Build messages and call LLM
            print("ðŸ¤– Generating answer...")
            messages = build_messages(
                query=question,
                context=context,
                history=[]
            )

            response = call_gemini(messages)

            print(f"   ðŸ’¬ Response: {len(response)} chars")

            # Store result
            result = {
                "id": question_id,
                "topic": topic,
                "question": question,
                "context_length": len(context),
                "sources_count": len(sources),
                "response_length": len(response),
                "response_preview": response[:200] + "..." if len(response) > 200 else response,
                "top_sources": [
                    {
                        "file": src.get("file_name", ""),
                        "page": src.get("page_number", ""),
                        "score": src.get("similarity_score", 0)
                    } for src in sources[:3]  # Top 3 sources
                ]
            }

            results.append(result)

            print("   âœ… Success")

        except Exception as e:
            print(f"   âŒ Failed: {e}")
            results.append({
                "id": question_id,
                "topic": topic,
                "question": question,
                "error": str(e)
            })

    return results

def save_results(results, filename="rag_test_results_part2.json"):
    """Save test results to JSON file"""
    output_path = Path(filename)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nðŸ’¾ Results saved to {output_path}")

def print_summary(results):
    """Print test summary"""
    successful = len([r for r in results if "error" not in r])
    failed = len([r for r in results if "error" in r])

    print(f"\nðŸ“Š Test Summary (Part 2):")
    print(f"   âœ… Successful: {successful}/5")
    print(f"   âŒ Failed: {failed}/5")

    if successful > 0:
        avg_context = sum(r.get("context_length", 0) for r in results if "error" not in r) / successful
        avg_response = sum(r.get("response_length", 0) for r in results if "error" not in r) / successful
        avg_sources = sum(r.get("sources_count", 0) for r in results if "error" not in r) / successful

        print(f"   ðŸ“„ Average context length: {avg_context:.0f} chars")
        print(f"   ðŸ’¬ Average response length: {avg_response:.0f} chars")
        print(f"   ðŸ“š Average sources count: {avg_sources:.1f}")

if __name__ == "__main__":
    results = test_rag_system_part2()
    save_results(results)
    print_summary(results)
    print("\nðŸŽ‰ RAG System Testing Part 2 Complete!")