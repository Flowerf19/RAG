# UI Module - Streamlit Frontend

## ğŸ¯ Má»¥c Ä‘Ã­ch
Module UI chá»©a toÃ n bá»™ Streamlit frontend, tÃ¡ch biá»‡t hoÃ n toÃ n khá»i logic LLM vÃ  backend.

## ğŸ—ï¸ Cáº¥u trÃºc
```
ui/
â”œâ”€â”€ app.py                   # Main Streamlit app (entry point)
â”œâ”€â”€ components/              # Reusable UI components
â”‚   â”œâ”€â”€ chat_display.py     # Chat message rendering
â”‚   â”œâ”€â”€ sidebar.py          # Sidebar controls and settings
â”‚   â””â”€â”€ source_display.py   # Source information display
â”œâ”€â”€ styles/
â”‚   â””â”€â”€ chat_styles.css     # CSS styling
â””â”€â”€ README.md
```

## ğŸš€ Cháº¡y UI
```powershell
# Tá»« thÆ° má»¥c gá»‘c cá»§a project
streamlit run ui/app.py

# Hoáº·c (náº¿u á»Ÿ trong thÆ° má»¥c ui/)
streamlit run app.py
```

## ğŸ”§ Architecture Pattern

### OOP Components
Má»—i UI component lÃ  má»™t class vá»›i responsibility rÃµ rÃ ng:

1. **ChatDisplay** - Render chat messages
   - `render(messages, is_generating, pending_prompt)`: Render chat log
   - `render_header(title)`: Render header

2. **SourceDisplay** - Display retrieval sources
   - `render(sources, retrieval_info, expanded_queries)`: Render all source info
   - `_render_retrieval_stats()`: Render metrics
   - `_render_sources()`: Render document sources

3. **Sidebar** - Sidebar controls
   - `render(on_embedding_clicked)`: Render sidebar and return settings
   - Returns settings dict: `{backend_mode, embedder_type, reranker_type, ...}`

### Main App Class
`RAGChatApp` orchestrates all components:
- Manages session state
- Coordinates UI flow
- Calls LLM clients (via factory)
- Calls backend retrieval (via `fetch_retrieval`)

## ğŸ“ NguyÃªn táº¯c thiáº¿t káº¿

### Single Responsibility
- **UI components**: Chá»‰ lo rendering, khÃ´ng cÃ³ business logic
- **Main app**: Orchestration vÃ  coordination
- **LLM clients**: Gá»i LLM (trong `llm/`)
- **Backend**: Retrieval logic (trong `pipeline/`)

### Dependency Injection
Components nháº­n dependencies qua constructor:
```python
sidebar = Sidebar(data_dir=paths_data_dir())
```

### Factory Pattern
LLM clients Ä‘Æ°á»£c táº¡o qua factory:
```python
client = LLMClientFactory.create_from_string(backend_mode)
response = client.generate(messages)
```

## ğŸ”— Integration vá»›i cÃ¡c module khÃ¡c

### LLM Module (`llm/`)
```python
from llm.client_factory import LLMClientFactory
from llm.chat_handler import build_messages

# Táº¡o client
client = LLMClientFactory.create_from_string("gemini")

# Build messages
messages = build_messages(query="Hello", context="...", history=[])

# Generate
response = client.generate(messages)
```

### Backend Module (`pipeline/`)
```python
from pipeline.backend_connector import fetch_retrieval

# Fetch context from retrieval
ret = fetch_retrieval(
    query_text="...",
    top_k=5,
    embedder_type="huggingface_local",
    reranker_type="bge_m3_hf_local"
)

context = ret["context"]
sources = ret["sources"]
```

## ğŸ“¦ Dependencies
- `streamlit`: UI framework
- `llm`: LLM clients (internal)
- `pipeline`: Backend retrieval (internal)

## ğŸ¨ Styling
CSS Ä‘Æ°á»£c load tá»« `styles/chat_styles.css` vá»›i cÃ¡c class:
- `.chat-header`: Chat window header
- `.chat-log`: Chat container
- `.chat-row`, `.chat-bubble`: Message styling
- `.typing`: Typing indicator animation

## ğŸ”„ Session State Management
App sá»­ dá»¥ng Streamlit session_state Ä‘á»ƒ quáº£n lÃ½:
- `messages`: Chat history (OpenAI format)
- `is_generating`: Generation status
- `pending_prompt`: Current prompt being processed
- `last_sources`: Last retrieval sources
- `last_retrieval_info`: Last retrieval metadata
- `last_queries`: Expanded queries from QEM

## âš ï¸ LÆ°u Ã½
- **KhÃ´ng gá»i LLM trá»±c tiáº¿p**: DÃ¹ng `LLMClientFactory`
- **KhÃ´ng gá»i embedding trá»±c tiáº¿p**: DÃ¹ng `fetch_retrieval`
- **Components thuáº§n tÃºy**: KhÃ´ng cÃ³ business logic trong UI components
